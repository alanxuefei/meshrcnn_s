{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cpu\")\n",
    "# Assuming the necessary imports are done and device is defined\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def display_input_image(img_input):\n",
    "    \"\"\"\n",
    "    Display the input image.\n",
    "\n",
    "    Parameters:\n",
    "    - img_input: A PyTorch tensor representing the image to be displayed.\n",
    "    \"\"\"\n",
    "    # Convert the PyTorch tensor to a NumPy array and transpose the axes for displaying\n",
    "    img_np = img_input.cpu().detach().numpy()\n",
    "    plt.imshow(img_np.transpose(1, 2, 0))\n",
    "    plt.title(\"Input Image\")\n",
    "    plt.axis('off')  # Hide axis for cleaner visualization\n",
    "    plt.show()\n",
    "\n",
    "def visualize_meshes(mesh_pred):\n",
    "    \"\"\"\n",
    "    Create visualizations for the predicted mesh.\n",
    "\n",
    "    Parameters:\n",
    "    - mesh_pred: A PyTorch3D Meshes object representing the predicted mesh.\n",
    "    \"\"\"\n",
    "    # Get vertices and faces from the predicted mesh\n",
    "    verts, faces = mesh_pred.get_mesh_verts_faces(0)\n",
    "    original_verts = verts.detach().numpy()\n",
    "    i, j, k = faces[:, 0].detach().numpy(), faces[:, 1].detach().numpy(), faces[:, 2].detach().numpy()\n",
    "\n",
    "    # Define the camera view\n",
    "    camera_view = dict(\n",
    "        eye=dict(x=0.0, y=0.0, z=2.0),\n",
    "        up=dict(x=0, y=1, z=0),\n",
    "        center=dict(x=0, y=0, z=0),\n",
    "        projection=dict(type='perspective')\n",
    "    )\n",
    "\n",
    "    # Create subplot figures for the predicted mesh\n",
    "    fig = make_subplots(\n",
    "        rows=1, cols=1,\n",
    "        specs=[[{'type': 'surface'}]],\n",
    "        subplot_titles=('Predicted Mesh',)\n",
    "    )\n",
    "\n",
    "    # Add predicted mesh to the subplot\n",
    "    fig.add_trace(\n",
    "        go.Mesh3d(\n",
    "            x=original_verts[:, 0], \n",
    "            y=original_verts[:, 1], \n",
    "            z=original_verts[:, 2], \n",
    "            i=i, \n",
    "            j=j, \n",
    "            k=k, \n",
    "            colorscale='Viridis', \n",
    "            opacity=0.50\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Update layout for the subplot\n",
    "    fig.update_layout(\n",
    "        autosize=False,\n",
    "        width=800,  # Adjusted for a single mesh visualization\n",
    "        height=600,\n",
    "        margin=dict(l=20, r=20, t=20, b=20),\n",
    "        paper_bgcolor=\"white\",\n",
    "        scene_camera=camera_view\n",
    "    )\n",
    "    \n",
    "    # Show the subplot figure\n",
    "    fig.show()\n",
    "    \n",
    "def visualize_mesh_with_cameras(mesh_gt, camera_poses):\n",
    "    \"\"\"\n",
    "    Visualize the ground truth mesh along with camera frustums to indicate camera positions and orientations.\n",
    "    \"\"\"\n",
    "    camera_view = dict(eye=dict(x=0.0, y=0.0, z=2.0), center=dict(x=0, y=0, z=0),\n",
    "                       up=dict(x=0, y=1, z=0), projection=dict(type='perspective'))\n",
    "    scene_layout = dict(xaxis=dict(range=[-3, 3]), yaxis=dict(range=[-3, 3]),\n",
    "                        zaxis=dict(range=[-3, 3]), camera=camera_view, aspectmode='cube')\n",
    "\n",
    "    verts_gt, faces_gt = mesh_gt.get_mesh_verts_faces(0)\n",
    "    mesh_trace_gt = go.Mesh3d(x=verts_gt[:, 0], y=verts_gt[:, 1], z=verts_gt[:, 2],\n",
    "                              i=faces_gt[:, 0], j=faces_gt[:, 1], k=faces_gt[:, 2],\n",
    "                              color='lightblue', opacity=0.5, name='GT Mesh')\n",
    "\n",
    "    fig = go.Figure(data=[mesh_trace_gt])\n",
    "\n",
    "    for index, RT in enumerate(camera_poses):\n",
    "        # Decompose the RT matrix to extract the position and the forward vector.\n",
    "        camera_position = RT[:3, 3].numpy()\n",
    "        forward_vector = RT[:3, 2].numpy()  # Forward direction (Z-axis)\n",
    "        end_point = camera_position + forward_vector * 0.3  # Scale the length of the arrow\n",
    "\n",
    "        # Add the camera position as a dot\n",
    "        fig.add_trace(go.Scatter3d(\n",
    "            x=[camera_position[0]],\n",
    "            y=[camera_position[1]],\n",
    "            z=[camera_position[2]],\n",
    "            mode='markers',\n",
    "            marker=dict(size=5, color='blue'),\n",
    "            name=f'Camera {index + 1}'\n",
    "        ))\n",
    "\n",
    "        # Add an arrow to represent the orientation\n",
    "        fig.add_trace(go.Scatter3d(\n",
    "            x=[camera_position[0], end_point[0]],\n",
    "            y=[camera_position[1], end_point[1]],\n",
    "            z=[camera_position[2], end_point[2]],\n",
    "            mode='lines+markers',\n",
    "            marker=dict(size=2, color='red'),\n",
    "            line=dict(color='red', width=2),\n",
    "            showlegend=False\n",
    "        ))\n",
    "\n",
    "        # Optionally, add cones to act as arrowheads\n",
    "        fig.add_trace(go.Cone(\n",
    "            x=[end_point[0]],\n",
    "            y=[end_point[1]],\n",
    "            z=[end_point[2]],\n",
    "            u=[forward_vector[0]],\n",
    "            v=[forward_vector[1]],\n",
    "            w=[forward_vector[2]],\n",
    "            sizemode='absolute',\n",
    "            sizeref=0.1,\n",
    "            anchor='tip',\n",
    "            showscale=False,\n",
    "            colorscale=[[0, 'red'], [1, 'red']],\n",
    "            name=f'Orientation {index + 1}'\n",
    "        ))\n",
    "\n",
    "    fig.update_layout(title=\"Ground Truth Mesh with Camera Frustums\", scene=scene_layout, autosize=False,\n",
    "                      width=800, height=600, margin=dict(l=50, r=50, b=50, t=50))\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    " import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models import vit_b_32, ViT_B_32_Weights\n",
    "import logging\n",
    "\n",
    "class ViTImageToRTNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # CNN Layer to be added before the ViT model\n",
    "        self.cnn_layer = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # ViT model setup, changed to use vit_b_32 with the default weights\n",
    "        self.vit = vit_b_32(weights=ViT_B_32_Weights.DEFAULT)  # Changed to vit_b_32\n",
    "        self.vit.heads = nn.Identity()  # Remove the classifier head\n",
    "        \n",
    "        # Regression layers\n",
    "        self.rotation_regressor = nn.Linear(768, 9)  # Assuming 768-dimensional output from ViT model\n",
    "        self.translation_regressor = nn.Linear(768, 3)  # Assuming the same for consistency\n",
    "\n",
    "        logging.info(\"ViT model initialized with removed classifier head, regressors added, and preceding CNN layer\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass input through CNN layer first\n",
    "        x = self.cnn_layer(x)\n",
    "        \n",
    "        # Pass the output of CNN to ViT model\n",
    "        features = self.vit(x)\n",
    "        cls_token = features\n",
    "        \n",
    "        # Compute rotation and translation\n",
    "        rotation = self.rotation_regressor(cls_token).view(-1, 3, 3)\n",
    "        translation = self.translation_regressor(cls_token).view(-1, 3, 1)\n",
    "        \n",
    "        # Concatenate to form the RT matrix\n",
    "        rt_matrix = torch.cat((rotation, translation), dim=2)\n",
    "        logging.debug(\"Model forward pass completed\")\n",
    "        return rt_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "anp_metadata": {
   "path": "fbsource/fbcode/vision/fair/pytorch3d/docs/tutorials/camera_position_optimization_with_differentiable_rendering.ipynb"
  },
  "bento_stylesheets": {
   "bento/extensions/flow/main.css": true,
   "bento/extensions/kernel_selector/main.css": true,
   "bento/extensions/kernel_ui/main.css": true,
   "bento/extensions/new_kernel/main.css": true,
   "bento/extensions/system_usage/main.css": true,
   "bento/extensions/theme/main.css": true
  },
  "colab": {
   "name": "camera_position_optimization_with_differentiable_rendering.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "disseminate_notebook_info": {
   "backup_notebook_id": "1062179640844868"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
