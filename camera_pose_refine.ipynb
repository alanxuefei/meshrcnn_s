{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available\n",
      "Tensor on GPU: tensor([1., 2., 3.], device='cuda:0')\n",
      "\n",
      "PyTorch3D is using CUDA\n"
     ]
    }
   ],
   "source": [
    "# Local utilities\n",
    "from util import *\n",
    "environment_check()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ViT_B_16_Weights.IMAGENET1K_V1`. You can also use `weights=ViT_B_16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference image tensor shape: (256, 256)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class PoseRefinementNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PoseRefinementNetwork, self).__init__()\n",
    "        # Load a pretrained ResNet model\n",
    "        self.feature_extractor = models.resnet18(pretrained=True)\n",
    "        \n",
    "        # Modify the first convolutional layer to accept 1-channel input\n",
    "        self.feature_extractor.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "        \n",
    "        # Remove the final fully connected layer\n",
    "        self.feature_extractor.fc = nn.Identity()\n",
    "        \n",
    "        # Transformer encoder for processing the features\n",
    "        # Adjust d_model to match the concatenated feature size (512 * 2 = 1024)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=1024, nhead=8)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=6)\n",
    "        \n",
    "        # Output layers for pose updates\n",
    "        # Adjust input features to 1024 to match the output of the transformer encoder\n",
    "        self.fc_translation = nn.Linear(1024, 3)  # Adjusted for translation update\n",
    "        self.fc_rotation = nn.Linear(1024, 4)  # Adjusted for rotation update (quaternion representation)\n",
    "\n",
    "    def forward(self, rendered_img, real_img_cropped):\n",
    "        # Extract features from both images\n",
    "        rendered_features = self.feature_extractor(rendered_img)\n",
    "        real_features = self.feature_extractor(real_img_cropped)\n",
    "        \n",
    "        # Concatenate features and prepare for transformer\n",
    "        combined_features = torch.cat((rendered_features, real_features), dim=1)\n",
    "        combined_features = combined_features.unsqueeze(0)  # Add batch dimension for transformer\n",
    "        \n",
    "        # Pass through transformer encoder\n",
    "        transformed_features = self.transformer_encoder(combined_features)\n",
    "        \n",
    "        # Predict translation and rotation updates\n",
    "        translation_update = self.fc_translation(transformed_features.squeeze(0))\n",
    "        #rotation_update = self.fc_rotation(transformed_features.squeeze(0))\n",
    "        \n",
    "        return translation_update, rotation_update\n",
    "    \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class PoseRefinementNetworkWithTransformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PoseRefinementNetworkWithTransformer, self).__init__()\n",
    "        # Load a pretrained ResNet model\n",
    "        self.feature_extractor = models.resnet18(pretrained=True)\n",
    "        \n",
    "        # Modify the first convolutional layer of the ResNet model to accept 1-channel input instead of the default 3\n",
    "        self.feature_extractor.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "        \n",
    "        # Remove the final fully connected layer to use ResNet as a feature extractor\n",
    "        self.feature_extractor.fc = nn.Identity()\n",
    "        \n",
    "        # Reduce the combined feature size from 1024 (512*2) to 512 to match the transformer's expected input size\n",
    "        self.feature_size_reducer = nn.Linear(1024, 512)\n",
    "        \n",
    "        # Define Transformer Encoder Layer with d_model=512\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=6)\n",
    "        \n",
    "        # Output layers for pose updates\n",
    "        self.fc_translation = nn.Linear(512, 3)\n",
    "        #self.fc_rotation = nn.Linear(512, 4)\n",
    "\n",
    "    def forward(self, rendered_img, real_img_cropped):\n",
    "        # Extract features from both images using the modified ResNet as the feature extractor\n",
    "        rendered_features = self.feature_extractor(rendered_img).flatten(start_dim=1)\n",
    "        real_features = self.feature_extractor(real_img_cropped).flatten(start_dim=1)\n",
    "        \n",
    "        # Concatenate features from both images\n",
    "        combined_features = torch.cat((rendered_features, real_features), dim=1)\n",
    "        \n",
    "        # Reduce the combined feature size to match the transformer's expected input size\n",
    "        reduced_features = self.feature_size_reducer(combined_features)\n",
    "        \n",
    "        # Add an extra dimension for the transformer\n",
    "        reduced_features = reduced_features.unsqueeze(1)\n",
    "        \n",
    "        # Transformer encoder with residual connection\n",
    "        # Adding the original reduced features to its transformed version\n",
    "        transformed_features = self.transformer_encoder(reduced_features)\n",
    "        # Ensure the original reduced_features is broadcastable to the transformed_features shape\n",
    "        residual_connection = reduced_features + transformed_features\n",
    "        \n",
    "        # Remove the sequence dimension\n",
    "        final_features = residual_connection.squeeze(1)\n",
    "        \n",
    "        # Predict translation and rotation updates\n",
    "        translation_update = self.fc_translation(final_features)\n",
    "        # rotation_update = self.fc_rotation(final_features)\n",
    "        \n",
    "        return translation_update  #rotation_update\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models import vit_b_16\n",
    "\n",
    "class PoseRefinementNetworkWithViT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PoseRefinementNetworkWithViT, self).__init__()\n",
    "        \n",
    "        # Initialize a Vision Transformer model with pretrained weights\n",
    "        self.feature_extractor = vit_b_16(pretrained=True)\n",
    "        \n",
    "        # Remove the classifier head to use the ViT as a feature extractor\n",
    "        self.feature_extractor.head = nn.Identity()\n",
    "        \n",
    "        # Adjust the input dimensions of the linear layers to match the actual feature size\n",
    "        feature_size = 1000  # Adjusted based on diagnostic output\n",
    "        \n",
    "        # Output layers for pose updates\n",
    "        self.fc_translation = nn.Linear(feature_size, 3)\n",
    "        #self.fc_rotation = nn.Linear(feature_size, 4)\n",
    "\n",
    "    def forward(self, rendered_img, real_img_cropped):\n",
    "        # Process images and extract features as before\n",
    "        rendered_img = self.prepare_image(rendered_img)\n",
    "        real_img_cropped = self.prepare_image(real_img_cropped)\n",
    "        \n",
    "        rendered_features = self.feature_extractor(rendered_img)\n",
    "        real_features = self.feature_extractor(real_img_cropped)\n",
    "        \n",
    "        # Now, correctly aggregate and predict updates based on adjusted feature size\n",
    "        translation_update = (self.fc_translation(rendered_features) + self.fc_translation(real_features)) / 2\n",
    "        #rotation_update = (self.fc_rotation(rendered_features) + self.fc_rotation(real_features)) / 2\n",
    "        \n",
    "        return translation_update#, rotation_update\n",
    "    \n",
    "    def prepare_image(self, img):\n",
    "        # Ensure the image has 3 channels by repeating the single channel\n",
    "        img_3ch = img.repeat(1, 3, 1, 1)\n",
    "        # Resize the image to match the expected input size of the ViT model (224x224)\n",
    "        img_resized = F.interpolate(img_3ch, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "        return img_resized\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "class PoseRefinementNetworkSimple(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PoseRefinementNetworkSimple, self).__init__()\n",
    "        # Load a pretrained ResNet model\n",
    "        self.feature_extractor = models.resnet18(pretrained=True)\n",
    "        \n",
    "        # Modify the first convolutional layer to accept 1-channel input\n",
    "        self.feature_extractor.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "        \n",
    "        # Remove the final fully connected layer\n",
    "        self.feature_extractor.fc = nn.Identity()\n",
    "        \n",
    "        # Instead of using a transformer, directly concatenate the features and use a linear layer\n",
    "        # Assuming that the feature size is 512 for each image from ResNet18 and we concatenate them\n",
    "        self.fc_combined = nn.Linear(512 * 2, 512)  # Combined feature layer\n",
    "        \n",
    "        # Output layers for pose updates, directly from the combined features\n",
    "        self.fc_translation = nn.Linear(512, 3)  # Adjusted for translation update\n",
    "        self.fc_rotation = nn.Linear(512, 4)  # Adjusted for rotation update (quaternion representation)\n",
    "\n",
    "    def forward(self, rendered_img, real_img_cropped):\n",
    "        # Extract features from both images\n",
    "        rendered_features = self.feature_extractor(rendered_img).flatten(start_dim=1)\n",
    "        real_features = self.feature_extractor(real_img_cropped).flatten(start_dim=1)\n",
    "        \n",
    "        # Concatenate features\n",
    "        combined_features = torch.cat((rendered_features, real_features), dim=1)\n",
    "        \n",
    "        # Pass concatenated features through a combined feature layer\n",
    "        combined_features = self.fc_combined(combined_features)\n",
    "        \n",
    "        # Predict translation and rotation updates\n",
    "        translation_update = self.fc_translation(combined_features)\n",
    "        rotation_update = self.fc_rotation(combined_features)\n",
    "        \n",
    "        return translation_update, rotation_update\n",
    "    \n",
    "import os\n",
    "import json\n",
    "import PIL.Image as Image\n",
    "import torch \n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "class PoseRefinementDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_json_filepath):\n",
    "        with open(data_json_filepath) as f:\n",
    "            self.data_json = json.load(f)\n",
    "        \n",
    "        # Define a transform to convert PIL images to tensors\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor(),  # Converts PIL Image or numpy.ndarray (H x W x C) in the range [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0]\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_json)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        entry = self.data_json[idx]\n",
    "\n",
    "        # Load rendered image as grayscale\n",
    "        rendered_img_path = entry['silhouette_path']  # Directly use the full path\n",
    "        rendered_img = Image.open(rendered_img_path).convert('L')\n",
    "\n",
    "        # Convert PIL images to tensors\n",
    "        rendered_img = self.transform(rendered_img)\n",
    "\n",
    "        # Ground truth pose \n",
    "        ground_truth_rt_delta = torch.tensor(entry['RT']) \n",
    "\n",
    "        return rendered_img, ground_truth_rt_delta\n",
    "\n",
    "    \n",
    "dataset = PoseRefinementDataset('./pose_refine_dataset/dataset_info.json')\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=True)   \n",
    " \n",
    "model = PoseRefinementNetworkWithViT().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001) \n",
    "\n",
    "from torchvision import transforms\n",
    "# Define the transformation pipeline\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Converts PIL Image or numpy.ndarray to tensor.\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize to [-1, 1] for each channel.\n",
    "])\n",
    "\n",
    "\n",
    " \n",
    "reference_img_path = \"./pose_refine_dataset/silhouettes/first_silhouette_0.png\"\n",
    "\n",
    "# Load reference image (grayscle)\n",
    "reference_img = Image.open(reference_img_path).convert('L') \n",
    "reference_img = reference_img\n",
    "# Convert the reference image to a tensor and add a batch dimension\n",
    "reference_img_tensor = transform(reference_img).unsqueeze(0)  # Now shape is [1, 1, H, W]\n",
    "\n",
    "print(f'Reference image tensor shape: {reference_img.size}')\n",
    "\n",
    "import kornia.geometry.conversions as conversions\n",
    "def rotation_matrix_to_quaternion(R):\n",
    "    \"\"\"\n",
    "    Convert a rotation matrix to a quaternion.\n",
    "    Assumes R is a batch of 3x3 rotation matrices.\n",
    "    \n",
    "    Parameters:\n",
    "    R (torch.Tensor): The input tensor containing batches of 3x3 rotation matrices.\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: The output tensor containing the corresponding quaternions.\n",
    "    \"\"\"\n",
    "    # Use kornia's function to convert rotation matrix to quaternion\n",
    "    quaternion = conversions.rotation_matrix_to_quaternion(R)\n",
    "    \n",
    "    return quaternion\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Adjust the path as necessary\n",
    "#model_path = \"./pose_refine_example/pose_refine_translate.pth\"\n",
    "\n",
    "# Load the model weights\n",
    "#state_dict = torch.load(model_path, map_location=device)\n",
    "\n",
    "# Apply the weights to your model instance\n",
    "#model.load_state_dict(state_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 0, Loss: 0.4129\n",
      "Epoch 2, Batch 0, Loss: 2.0261\n",
      "Epoch 3, Batch 0, Loss: 1.8677\n",
      "Epoch 4, Batch 0, Loss: 1.4348\n",
      "Epoch 5, Batch 0, Loss: 0.3224\n",
      "Epoch 6, Batch 0, Loss: 1.0744\n",
      "Epoch 7, Batch 0, Loss: 0.5163\n",
      "Epoch 8, Batch 0, Loss: 0.9598\n",
      "Epoch 9, Batch 0, Loss: 0.8229\n",
      "Epoch 10, Batch 0, Loss: 3.5334\n",
      "Epoch 11, Batch 0, Loss: 0.7338\n",
      "Epoch 12, Batch 0, Loss: 0.4635\n",
      "Epoch 13, Batch 0, Loss: 0.4346\n",
      "Epoch 14, Batch 0, Loss: 0.6101\n",
      "Epoch 15, Batch 0, Loss: 1.1971\n",
      "Epoch 16, Batch 0, Loss: 0.5585\n",
      "Epoch 17, Batch 0, Loss: 0.3604\n",
      "Epoch 18, Batch 0, Loss: 0.5580\n",
      "Epoch 19, Batch 0, Loss: 0.6496\n",
      "Epoch 20, Batch 0, Loss: 0.3987\n",
      "Epoch 21, Batch 0, Loss: 0.5975\n",
      "Epoch 22, Batch 0, Loss: 0.6741\n",
      "Epoch 23, Batch 0, Loss: 0.3838\n",
      "Epoch 24, Batch 0, Loss: 1.0974\n",
      "Epoch 25, Batch 0, Loss: 0.9105\n",
      "Epoch 26, Batch 0, Loss: 0.6273\n",
      "Epoch 27, Batch 0, Loss: 0.7929\n",
      "Epoch 28, Batch 0, Loss: 0.8985\n",
      "Epoch 29, Batch 0, Loss: 0.7687\n",
      "Epoch 30, Batch 0, Loss: 0.7529\n",
      "Epoch 31, Batch 0, Loss: 0.7260\n",
      "Epoch 32, Batch 0, Loss: 0.6457\n",
      "Epoch 33, Batch 0, Loss: 0.4813\n",
      "Epoch 34, Batch 0, Loss: 0.5141\n",
      "Epoch 35, Batch 0, Loss: 0.5675\n",
      "Epoch 36, Batch 0, Loss: 0.6070\n",
      "Epoch 37, Batch 0, Loss: 1.5661\n",
      "Epoch 38, Batch 0, Loss: 1.0853\n",
      "Epoch 39, Batch 0, Loss: 0.5426\n",
      "Epoch 40, Batch 0, Loss: 0.6492\n",
      "Epoch 41, Batch 0, Loss: 1.9944\n",
      "Epoch 42, Batch 0, Loss: 0.6736\n",
      "Epoch 43, Batch 0, Loss: 0.7049\n",
      "Epoch 44, Batch 0, Loss: 0.4224\n",
      "Epoch 45, Batch 0, Loss: 0.8473\n",
      "Epoch 46, Batch 0, Loss: 0.6846\n",
      "Epoch 47, Batch 0, Loss: 0.6941\n",
      "Epoch 48, Batch 0, Loss: 0.7036\n",
      "Epoch 49, Batch 0, Loss: 0.2321\n",
      "Epoch 50, Batch 0, Loss: 0.3299\n",
      "Epoch 51, Batch 0, Loss: 0.6567\n",
      "Epoch 52, Batch 0, Loss: 0.5978\n",
      "Epoch 53, Batch 0, Loss: 0.4162\n",
      "Epoch 54, Batch 0, Loss: 0.4920\n",
      "Epoch 55, Batch 0, Loss: 0.5840\n",
      "Epoch 56, Batch 0, Loss: 0.4946\n",
      "Epoch 57, Batch 0, Loss: 0.5094\n",
      "Epoch 58, Batch 0, Loss: 0.9997\n",
      "Epoch 59, Batch 0, Loss: 0.4360\n",
      "Epoch 60, Batch 0, Loss: 0.4215\n",
      "Epoch 61, Batch 0, Loss: 0.2950\n",
      "Epoch 62, Batch 0, Loss: 0.7794\n",
      "Epoch 63, Batch 0, Loss: 0.4385\n",
      "Epoch 64, Batch 0, Loss: 1.8035\n",
      "Epoch 65, Batch 0, Loss: 2.1642\n",
      "Epoch 66, Batch 0, Loss: 0.6673\n",
      "Epoch 67, Batch 0, Loss: 0.8091\n",
      "Epoch 68, Batch 0, Loss: 0.6017\n",
      "Epoch 69, Batch 0, Loss: 0.6835\n",
      "Epoch 70, Batch 0, Loss: 0.7236\n",
      "Epoch 71, Batch 0, Loss: 0.4363\n",
      "Epoch 72, Batch 0, Loss: 2.0716\n",
      "Epoch 73, Batch 0, Loss: 1.5696\n",
      "Epoch 74, Batch 0, Loss: 0.6709\n",
      "Epoch 75, Batch 0, Loss: 0.3369\n",
      "Epoch 76, Batch 0, Loss: 0.4404\n",
      "Epoch 77, Batch 0, Loss: 2.1578\n",
      "Epoch 78, Batch 0, Loss: 0.6515\n",
      "Epoch 79, Batch 0, Loss: 0.6169\n",
      "Epoch 80, Batch 0, Loss: 0.4626\n",
      "Epoch 81, Batch 0, Loss: 0.3454\n",
      "Epoch 82, Batch 0, Loss: 0.3845\n",
      "Epoch 83, Batch 0, Loss: 0.5422\n",
      "Epoch 84, Batch 0, Loss: 0.5410\n",
      "Epoch 85, Batch 0, Loss: 0.5498\n",
      "Epoch 86, Batch 0, Loss: 0.3890\n",
      "Epoch 87, Batch 0, Loss: 0.8843\n",
      "Epoch 88, Batch 0, Loss: 0.6313\n",
      "Epoch 89, Batch 0, Loss: 0.5671\n",
      "Epoch 90, Batch 0, Loss: 0.7423\n",
      "Epoch 91, Batch 0, Loss: 1.8707\n",
      "Epoch 92, Batch 0, Loss: 0.4332\n",
      "Epoch 93, Batch 0, Loss: 0.7118\n",
      "Epoch 94, Batch 0, Loss: 0.4478\n",
      "Epoch 95, Batch 0, Loss: 0.9352\n",
      "Epoch 96, Batch 0, Loss: 0.4828\n",
      "Epoch 97, Batch 0, Loss: 0.6210\n",
      "Epoch 98, Batch 0, Loss: 1.8068\n",
      "Epoch 99, Batch 0, Loss: 0.8242\n",
      "Epoch 100, Batch 0, Loss: 0.4519\n",
      "Epoch 101, Batch 0, Loss: 0.5331\n",
      "Epoch 102, Batch 0, Loss: 0.4901\n",
      "Epoch 103, Batch 0, Loss: 0.4953\n",
      "Epoch 104, Batch 0, Loss: 0.7349\n",
      "Epoch 105, Batch 0, Loss: 1.5954\n",
      "Epoch 106, Batch 0, Loss: 0.4643\n",
      "Epoch 107, Batch 0, Loss: 0.7108\n",
      "Epoch 108, Batch 0, Loss: 0.6296\n",
      "Epoch 109, Batch 0, Loss: 0.9812\n",
      "Epoch 110, Batch 0, Loss: 0.9332\n",
      "Epoch 111, Batch 0, Loss: 0.6113\n",
      "Epoch 112, Batch 0, Loss: 0.8711\n",
      "Epoch 113, Batch 0, Loss: 0.4782\n",
      "Epoch 114, Batch 0, Loss: 0.6112\n",
      "Epoch 115, Batch 0, Loss: 0.7365\n",
      "Epoch 116, Batch 0, Loss: 0.4110\n",
      "Epoch 117, Batch 0, Loss: 0.4412\n",
      "Epoch 118, Batch 0, Loss: 0.3635\n",
      "Epoch 119, Batch 0, Loss: 0.5098\n",
      "Epoch 120, Batch 0, Loss: 0.6503\n",
      "Epoch 121, Batch 0, Loss: 0.5388\n",
      "Epoch 122, Batch 0, Loss: 0.9103\n",
      "Epoch 123, Batch 0, Loss: 1.9864\n",
      "Epoch 124, Batch 0, Loss: 0.5702\n",
      "Epoch 125, Batch 0, Loss: 0.9253\n",
      "Epoch 126, Batch 0, Loss: 0.4629\n",
      "Epoch 127, Batch 0, Loss: 0.6449\n",
      "Epoch 128, Batch 0, Loss: 0.8599\n",
      "Epoch 129, Batch 0, Loss: 0.2660\n",
      "Epoch 130, Batch 0, Loss: 0.5884\n",
      "Epoch 131, Batch 0, Loss: 1.2281\n",
      "Epoch 132, Batch 0, Loss: 0.8337\n",
      "Epoch 133, Batch 0, Loss: 0.4516\n",
      "Epoch 134, Batch 0, Loss: 1.6911\n",
      "Epoch 135, Batch 0, Loss: 0.5965\n",
      "Epoch 136, Batch 0, Loss: 0.7078\n",
      "Epoch 137, Batch 0, Loss: 0.9913\n",
      "Epoch 138, Batch 0, Loss: 0.7593\n",
      "Epoch 139, Batch 0, Loss: 0.4658\n",
      "Epoch 140, Batch 0, Loss: 0.6995\n",
      "Epoch 141, Batch 0, Loss: 0.6624\n",
      "Epoch 142, Batch 0, Loss: 0.2022\n",
      "Epoch 143, Batch 0, Loss: 0.5957\n",
      "Epoch 144, Batch 0, Loss: 0.9010\n",
      "Epoch 145, Batch 0, Loss: 0.4274\n",
      "Epoch 146, Batch 0, Loss: 0.5941\n",
      "Epoch 147, Batch 0, Loss: 0.4717\n",
      "Epoch 148, Batch 0, Loss: 0.5535\n",
      "Epoch 149, Batch 0, Loss: 0.3289\n",
      "Epoch 150, Batch 0, Loss: 0.6446\n",
      "Epoch 151, Batch 0, Loss: 0.5974\n",
      "Epoch 152, Batch 0, Loss: 0.4670\n",
      "Epoch 153, Batch 0, Loss: 2.1983\n",
      "Epoch 154, Batch 0, Loss: 0.6833\n",
      "Epoch 155, Batch 0, Loss: 0.8229\n",
      "Epoch 156, Batch 0, Loss: 0.4356\n",
      "Epoch 157, Batch 0, Loss: 0.5975\n",
      "Epoch 158, Batch 0, Loss: 1.8126\n",
      "Epoch 159, Batch 0, Loss: 1.9062\n",
      "Epoch 160, Batch 0, Loss: 0.4495\n",
      "Epoch 161, Batch 0, Loss: 0.5998\n",
      "Epoch 162, Batch 0, Loss: 0.3630\n",
      "Epoch 163, Batch 0, Loss: 0.8357\n",
      "Epoch 164, Batch 0, Loss: 0.6035\n",
      "Epoch 165, Batch 0, Loss: 0.6001\n",
      "Epoch 166, Batch 0, Loss: 0.5336\n",
      "Epoch 167, Batch 0, Loss: 0.6970\n",
      "Epoch 168, Batch 0, Loss: 0.7834\n",
      "Epoch 169, Batch 0, Loss: 0.4722\n",
      "Epoch 170, Batch 0, Loss: 0.8200\n",
      "Epoch 171, Batch 0, Loss: 0.5398\n",
      "Epoch 172, Batch 0, Loss: 0.4704\n",
      "Epoch 173, Batch 0, Loss: 0.3069\n",
      "Epoch 174, Batch 0, Loss: 0.4372\n",
      "Epoch 175, Batch 0, Loss: 0.8713\n",
      "Epoch 176, Batch 0, Loss: 0.5492\n",
      "Epoch 177, Batch 0, Loss: 0.6983\n",
      "Epoch 178, Batch 0, Loss: 0.4362\n",
      "Epoch 179, Batch 0, Loss: 0.3797\n",
      "Epoch 180, Batch 0, Loss: 0.7414\n",
      "Epoch 181, Batch 0, Loss: 0.7036\n",
      "Epoch 182, Batch 0, Loss: 0.3575\n",
      "Epoch 183, Batch 0, Loss: 0.8048\n",
      "Epoch 184, Batch 0, Loss: 0.6869\n",
      "Epoch 185, Batch 0, Loss: 0.4496\n",
      "Epoch 186, Batch 0, Loss: 0.8893\n",
      "Epoch 187, Batch 0, Loss: 0.4702\n",
      "Epoch 188, Batch 0, Loss: 0.5965\n",
      "Epoch 189, Batch 0, Loss: 0.8471\n",
      "Epoch 190, Batch 0, Loss: 0.7821\n",
      "Epoch 191, Batch 0, Loss: 0.4551\n",
      "Epoch 192, Batch 0, Loss: 0.2949\n",
      "Epoch 193, Batch 0, Loss: 0.5747\n",
      "Epoch 194, Batch 0, Loss: 0.7677\n",
      "Epoch 195, Batch 0, Loss: 0.6839\n",
      "Epoch 196, Batch 0, Loss: 0.6684\n",
      "Epoch 197, Batch 0, Loss: 0.4651\n",
      "Epoch 198, Batch 0, Loss: 0.8869\n",
      "Epoch 199, Batch 0, Loss: 0.6808\n",
      "Epoch 200, Batch 0, Loss: 0.4158\n",
      "Epoch 201, Batch 0, Loss: 0.3158\n",
      "Epoch 202, Batch 0, Loss: 0.3970\n",
      "Epoch 203, Batch 0, Loss: 0.7925\n",
      "Epoch 204, Batch 0, Loss: 0.6840\n",
      "Epoch 205, Batch 0, Loss: 0.6583\n",
      "Epoch 206, Batch 0, Loss: 0.7871\n",
      "Epoch 207, Batch 0, Loss: 0.3437\n",
      "Epoch 208, Batch 0, Loss: 0.6132\n",
      "Epoch 209, Batch 0, Loss: 0.4893\n",
      "Epoch 210, Batch 0, Loss: 0.4577\n",
      "Epoch 211, Batch 0, Loss: 0.3651\n",
      "Epoch 212, Batch 0, Loss: 0.6713\n",
      "Epoch 213, Batch 0, Loss: 0.3287\n",
      "Epoch 214, Batch 0, Loss: 0.5806\n",
      "Epoch 215, Batch 0, Loss: 0.8335\n",
      "Epoch 216, Batch 0, Loss: 0.3011\n",
      "Epoch 217, Batch 0, Loss: 0.5630\n",
      "Epoch 218, Batch 0, Loss: 0.7088\n",
      "Epoch 219, Batch 0, Loss: 0.6539\n",
      "Epoch 220, Batch 0, Loss: 0.6449\n",
      "Epoch 221, Batch 0, Loss: 0.6076\n",
      "Epoch 222, Batch 0, Loss: 0.8387\n",
      "Epoch 223, Batch 0, Loss: 0.7560\n",
      "Epoch 224, Batch 0, Loss: 0.7178\n",
      "Epoch 225, Batch 0, Loss: 0.5495\n",
      "Epoch 226, Batch 0, Loss: 0.3552\n",
      "Epoch 227, Batch 0, Loss: 0.9382\n",
      "Epoch 228, Batch 0, Loss: 0.7092\n",
      "Epoch 229, Batch 0, Loss: 0.5978\n",
      "Epoch 230, Batch 0, Loss: 0.4309\n",
      "Epoch 231, Batch 0, Loss: 0.6671\n",
      "Epoch 232, Batch 0, Loss: 0.4462\n",
      "Epoch 233, Batch 0, Loss: 2.0566\n",
      "Epoch 234, Batch 0, Loss: 0.4097\n",
      "Epoch 235, Batch 0, Loss: 2.0217\n",
      "Epoch 236, Batch 0, Loss: 0.6185\n",
      "Epoch 237, Batch 0, Loss: 0.7006\n",
      "Epoch 238, Batch 0, Loss: 0.4730\n",
      "Epoch 239, Batch 0, Loss: 0.4236\n",
      "Epoch 240, Batch 0, Loss: 0.3244\n",
      "Epoch 241, Batch 0, Loss: 0.6974\n",
      "Epoch 242, Batch 0, Loss: 0.6051\n",
      "Epoch 243, Batch 0, Loss: 0.8346\n",
      "Epoch 244, Batch 0, Loss: 0.6991\n",
      "Epoch 245, Batch 0, Loss: 0.7847\n",
      "Epoch 246, Batch 0, Loss: 0.8786\n",
      "Epoch 247, Batch 0, Loss: 0.4057\n",
      "Epoch 248, Batch 0, Loss: 0.3457\n",
      "Epoch 249, Batch 0, Loss: 0.7213\n",
      "Epoch 250, Batch 0, Loss: 0.4785\n",
      "Epoch 251, Batch 0, Loss: 0.5298\n",
      "Epoch 252, Batch 0, Loss: 0.8638\n",
      "Epoch 253, Batch 0, Loss: 0.9046\n",
      "Epoch 254, Batch 0, Loss: 2.0139\n",
      "Epoch 255, Batch 0, Loss: 0.3584\n",
      "Epoch 256, Batch 0, Loss: 0.5548\n",
      "Epoch 257, Batch 0, Loss: 0.5343\n",
      "Epoch 258, Batch 0, Loss: 0.5805\n",
      "Epoch 259, Batch 0, Loss: 0.7465\n",
      "Epoch 260, Batch 0, Loss: 0.3265\n",
      "Epoch 261, Batch 0, Loss: 0.7690\n",
      "Epoch 262, Batch 0, Loss: 0.6089\n",
      "Epoch 263, Batch 0, Loss: 0.5574\n",
      "Epoch 264, Batch 0, Loss: 0.7136\n",
      "Epoch 265, Batch 0, Loss: 0.4111\n",
      "Epoch 266, Batch 0, Loss: 0.7304\n",
      "Epoch 267, Batch 0, Loss: 0.4223\n",
      "Epoch 268, Batch 0, Loss: 0.3714\n",
      "Epoch 269, Batch 0, Loss: 1.9700\n",
      "Epoch 270, Batch 0, Loss: 0.4058\n",
      "Epoch 271, Batch 0, Loss: 0.7206\n",
      "Epoch 272, Batch 0, Loss: 0.4676\n",
      "Epoch 273, Batch 0, Loss: 0.4616\n",
      "Epoch 274, Batch 0, Loss: 0.4209\n",
      "Epoch 275, Batch 0, Loss: 0.6937\n",
      "Epoch 276, Batch 0, Loss: 0.4451\n",
      "Epoch 277, Batch 0, Loss: 0.7263\n",
      "Epoch 278, Batch 0, Loss: 1.9615\n",
      "Epoch 279, Batch 0, Loss: 0.4630\n",
      "Epoch 280, Batch 0, Loss: 0.5879\n",
      "Epoch 281, Batch 0, Loss: 0.3399\n",
      "Epoch 282, Batch 0, Loss: 0.6418\n",
      "Epoch 283, Batch 0, Loss: 0.4670\n",
      "Epoch 284, Batch 0, Loss: 0.7423\n",
      "Epoch 285, Batch 0, Loss: 0.4834\n",
      "Epoch 286, Batch 0, Loss: 0.5146\n",
      "Epoch 287, Batch 0, Loss: 0.6446\n",
      "Epoch 288, Batch 0, Loss: 0.4830\n",
      "Epoch 289, Batch 0, Loss: 2.0047\n",
      "Epoch 290, Batch 0, Loss: 0.8251\n",
      "Epoch 291, Batch 0, Loss: 0.3207\n",
      "Epoch 292, Batch 0, Loss: 0.5508\n",
      "Epoch 293, Batch 0, Loss: 0.7599\n",
      "Epoch 294, Batch 0, Loss: 0.3831\n",
      "Epoch 295, Batch 0, Loss: 0.5787\n",
      "Epoch 296, Batch 0, Loss: 0.4730\n",
      "Epoch 297, Batch 0, Loss: 0.4026\n",
      "Epoch 298, Batch 0, Loss: 0.8363\n",
      "Epoch 299, Batch 0, Loss: 0.6123\n",
      "Epoch 300, Batch 0, Loss: 0.7142\n",
      "Epoch 301, Batch 0, Loss: 0.6419\n",
      "Epoch 302, Batch 0, Loss: 0.3020\n",
      "Epoch 303, Batch 0, Loss: 0.6052\n",
      "Epoch 304, Batch 0, Loss: 0.6143\n",
      "Epoch 305, Batch 0, Loss: 2.0104\n",
      "Epoch 306, Batch 0, Loss: 0.4519\n",
      "Epoch 307, Batch 0, Loss: 2.0238\n",
      "Epoch 308, Batch 0, Loss: 1.9882\n",
      "Epoch 309, Batch 0, Loss: 0.5691\n",
      "Epoch 310, Batch 0, Loss: 0.4085\n",
      "Epoch 311, Batch 0, Loss: 0.5776\n",
      "Epoch 312, Batch 0, Loss: 0.4566\n",
      "Epoch 313, Batch 0, Loss: 0.3895\n",
      "Epoch 314, Batch 0, Loss: 0.6517\n",
      "Epoch 315, Batch 0, Loss: 0.4463\n",
      "Epoch 316, Batch 0, Loss: 0.7358\n",
      "Epoch 317, Batch 0, Loss: 0.6287\n",
      "Epoch 318, Batch 0, Loss: 0.5069\n",
      "Epoch 319, Batch 0, Loss: 1.9107\n",
      "Epoch 320, Batch 0, Loss: 0.5747\n",
      "Epoch 321, Batch 0, Loss: 0.3720\n",
      "Epoch 322, Batch 0, Loss: 4.2007\n",
      "Epoch 323, Batch 0, Loss: 2.1783\n",
      "Epoch 324, Batch 0, Loss: 2.0751\n",
      "Epoch 325, Batch 0, Loss: 5.5714\n",
      "Epoch 326, Batch 0, Loss: 2.9709\n",
      "Epoch 327, Batch 0, Loss: 0.4731\n",
      "Epoch 328, Batch 0, Loss: 0.5688\n",
      "Epoch 329, Batch 0, Loss: 0.9549\n",
      "Epoch 330, Batch 0, Loss: 0.8722\n",
      "Epoch 331, Batch 0, Loss: 0.3357\n",
      "Epoch 332, Batch 0, Loss: 0.3738\n",
      "Epoch 333, Batch 0, Loss: 0.4211\n",
      "Epoch 334, Batch 0, Loss: 0.5013\n",
      "Epoch 335, Batch 0, Loss: 1.9924\n",
      "Epoch 336, Batch 0, Loss: 1.5500\n",
      "Epoch 337, Batch 0, Loss: 0.5455\n",
      "Epoch 338, Batch 0, Loss: 0.4878\n",
      "Epoch 339, Batch 0, Loss: 0.4027\n",
      "Epoch 340, Batch 0, Loss: 0.6736\n",
      "Epoch 341, Batch 0, Loss: 0.5836\n",
      "Epoch 342, Batch 0, Loss: 2.0514\n",
      "Epoch 343, Batch 0, Loss: 0.6882\n",
      "Epoch 344, Batch 0, Loss: 1.4317\n",
      "Epoch 345, Batch 0, Loss: 0.4227\n",
      "Epoch 346, Batch 0, Loss: 0.4347\n",
      "Epoch 347, Batch 0, Loss: 0.5878\n",
      "Epoch 348, Batch 0, Loss: 0.8243\n",
      "Epoch 349, Batch 0, Loss: 0.4902\n",
      "Epoch 350, Batch 0, Loss: 0.7181\n",
      "Epoch 351, Batch 0, Loss: 0.0189\n",
      "Epoch 352, Batch 0, Loss: 0.5183\n",
      "Epoch 353, Batch 0, Loss: 0.6208\n",
      "Epoch 354, Batch 0, Loss: 0.6495\n",
      "Epoch 355, Batch 0, Loss: 0.5459\n",
      "Epoch 356, Batch 0, Loss: 0.4900\n",
      "Epoch 357, Batch 0, Loss: 0.4843\n",
      "Epoch 358, Batch 0, Loss: 0.5702\n",
      "Epoch 359, Batch 0, Loss: 0.4944\n",
      "Epoch 360, Batch 0, Loss: 0.4820\n",
      "Epoch 361, Batch 0, Loss: 0.4851\n",
      "Epoch 362, Batch 0, Loss: 0.6734\n",
      "Epoch 363, Batch 0, Loss: 0.4398\n",
      "Epoch 364, Batch 0, Loss: 0.0056\n",
      "Epoch 365, Batch 0, Loss: 0.5331\n",
      "Epoch 366, Batch 0, Loss: 0.7289\n",
      "Epoch 367, Batch 0, Loss: 0.0057\n",
      "Epoch 368, Batch 0, Loss: 0.3929\n",
      "Epoch 369, Batch 0, Loss: 0.5775\n",
      "Epoch 370, Batch 0, Loss: 0.5798\n",
      "Epoch 371, Batch 0, Loss: 0.5201\n",
      "Epoch 372, Batch 0, Loss: 0.6243\n",
      "Epoch 373, Batch 0, Loss: 0.6019\n",
      "Epoch 374, Batch 0, Loss: 0.5313\n",
      "Epoch 375, Batch 0, Loss: 0.5448\n",
      "Epoch 376, Batch 0, Loss: 0.6738\n",
      "Epoch 377, Batch 0, Loss: 0.3683\n",
      "Epoch 378, Batch 0, Loss: 0.5227\n",
      "Epoch 379, Batch 0, Loss: 0.0070\n",
      "Epoch 380, Batch 0, Loss: 0.5603\n",
      "Epoch 381, Batch 0, Loss: 0.5361\n",
      "Epoch 382, Batch 0, Loss: 0.6333\n",
      "Epoch 383, Batch 0, Loss: 0.6629\n",
      "Epoch 384, Batch 0, Loss: 0.4832\n",
      "Epoch 385, Batch 0, Loss: 0.0023\n",
      "Epoch 386, Batch 0, Loss: 0.5446\n",
      "Epoch 387, Batch 0, Loss: 0.5047\n",
      "Epoch 388, Batch 0, Loss: 0.5937\n",
      "Epoch 389, Batch 0, Loss: 0.5661\n",
      "Epoch 390, Batch 0, Loss: 0.0002\n",
      "Epoch 391, Batch 0, Loss: 0.6489\n",
      "Epoch 392, Batch 0, Loss: 0.4144\n",
      "Epoch 393, Batch 0, Loss: 0.5187\n",
      "Epoch 394, Batch 0, Loss: 0.6472\n",
      "Epoch 395, Batch 0, Loss: 0.4998\n",
      "Epoch 396, Batch 0, Loss: 0.0533\n",
      "Epoch 397, Batch 0, Loss: 0.7638\n",
      "Epoch 398, Batch 0, Loss: 0.7665\n",
      "Epoch 399, Batch 0, Loss: 0.4353\n",
      "Epoch 400, Batch 0, Loss: 0.5120\n",
      "Epoch 401, Batch 0, Loss: 0.7763\n",
      "Epoch 402, Batch 0, Loss: 0.8179\n",
      "Epoch 403, Batch 0, Loss: 0.6087\n",
      "Epoch 404, Batch 0, Loss: 0.5532\n",
      "Epoch 405, Batch 0, Loss: 0.4086\n",
      "Epoch 406, Batch 0, Loss: 0.8550\n",
      "Epoch 407, Batch 0, Loss: 0.5676\n",
      "Epoch 408, Batch 0, Loss: 2.8982\n",
      "Epoch 409, Batch 0, Loss: 0.3077\n",
      "Epoch 410, Batch 0, Loss: 0.5996\n",
      "Epoch 411, Batch 0, Loss: 0.1912\n",
      "Epoch 412, Batch 0, Loss: 0.5525\n",
      "Epoch 413, Batch 0, Loss: 0.5372\n",
      "Epoch 414, Batch 0, Loss: 0.2597\n",
      "Epoch 415, Batch 0, Loss: 0.5381\n",
      "Epoch 416, Batch 0, Loss: 2.1806\n",
      "Epoch 417, Batch 0, Loss: 0.5879\n",
      "Epoch 418, Batch 0, Loss: 0.6549\n",
      "Epoch 419, Batch 0, Loss: 2.0556\n",
      "Epoch 420, Batch 0, Loss: 0.8305\n",
      "Epoch 421, Batch 0, Loss: 0.4238\n",
      "Epoch 422, Batch 0, Loss: 0.7156\n",
      "Epoch 423, Batch 0, Loss: 0.5567\n",
      "Epoch 424, Batch 0, Loss: 0.4630\n",
      "Epoch 425, Batch 0, Loss: 0.6161\n",
      "Epoch 426, Batch 0, Loss: 0.6755\n",
      "Epoch 427, Batch 0, Loss: 0.8738\n",
      "Epoch 428, Batch 0, Loss: 0.4673\n",
      "Epoch 429, Batch 0, Loss: 0.2854\n",
      "Epoch 430, Batch 0, Loss: 0.4481\n",
      "Epoch 431, Batch 0, Loss: 0.4331\n",
      "Epoch 432, Batch 0, Loss: 0.4384\n",
      "Epoch 433, Batch 0, Loss: 0.3876\n",
      "Epoch 434, Batch 0, Loss: 0.4841\n",
      "Epoch 435, Batch 0, Loss: 0.3205\n",
      "Epoch 436, Batch 0, Loss: 0.5610\n",
      "Epoch 437, Batch 0, Loss: 0.4620\n",
      "Epoch 438, Batch 0, Loss: 0.9101\n",
      "Epoch 439, Batch 0, Loss: 1.7915\n",
      "Epoch 440, Batch 0, Loss: 0.6054\n",
      "Epoch 441, Batch 0, Loss: 0.6023\n",
      "Epoch 442, Batch 0, Loss: 0.5486\n",
      "Epoch 443, Batch 0, Loss: 0.6667\n",
      "Epoch 444, Batch 0, Loss: 0.6093\n",
      "Epoch 445, Batch 0, Loss: 0.6477\n",
      "Epoch 446, Batch 0, Loss: 0.8778\n",
      "Epoch 447, Batch 0, Loss: 0.3753\n",
      "Epoch 448, Batch 0, Loss: 0.6331\n",
      "Epoch 449, Batch 0, Loss: 0.4767\n",
      "Epoch 450, Batch 0, Loss: 0.6560\n",
      "Epoch 451, Batch 0, Loss: 0.6094\n",
      "Epoch 452, Batch 0, Loss: 0.4534\n",
      "Epoch 453, Batch 0, Loss: 0.6441\n",
      "Epoch 454, Batch 0, Loss: 0.5447\n",
      "Epoch 455, Batch 0, Loss: 0.5926\n",
      "Epoch 456, Batch 0, Loss: 0.6454\n",
      "Epoch 457, Batch 0, Loss: 0.5003\n",
      "Epoch 458, Batch 0, Loss: 0.5519\n",
      "Epoch 459, Batch 0, Loss: 0.4658\n",
      "Epoch 460, Batch 0, Loss: 0.5252\n",
      "Epoch 461, Batch 0, Loss: 0.5002\n",
      "Epoch 462, Batch 0, Loss: 0.6035\n",
      "Epoch 463, Batch 0, Loss: 0.5492\n",
      "Epoch 464, Batch 0, Loss: 0.6237\n",
      "Epoch 465, Batch 0, Loss: 0.4760\n",
      "Epoch 466, Batch 0, Loss: 0.5820\n",
      "Epoch 467, Batch 0, Loss: 0.6159\n",
      "Epoch 468, Batch 0, Loss: 0.7047\n",
      "Epoch 469, Batch 0, Loss: 0.0196\n",
      "Epoch 470, Batch 0, Loss: 0.5265\n",
      "Epoch 471, Batch 0, Loss: 0.4995\n",
      "Epoch 472, Batch 0, Loss: 0.5238\n",
      "Epoch 473, Batch 0, Loss: 0.2061\n",
      "Epoch 474, Batch 0, Loss: 0.1072\n",
      "Epoch 475, Batch 0, Loss: 0.3270\n",
      "Epoch 476, Batch 0, Loss: 0.5953\n",
      "Epoch 477, Batch 0, Loss: 0.7660\n",
      "Epoch 478, Batch 0, Loss: 0.5249\n",
      "Epoch 479, Batch 0, Loss: 0.5241\n",
      "Epoch 480, Batch 0, Loss: 0.1049\n",
      "Epoch 481, Batch 0, Loss: 0.5145\n",
      "Epoch 482, Batch 0, Loss: 0.5419\n",
      "Epoch 483, Batch 0, Loss: 0.4571\n",
      "Epoch 484, Batch 0, Loss: 0.5525\n",
      "Epoch 485, Batch 0, Loss: 0.5759\n",
      "Epoch 486, Batch 0, Loss: 0.7014\n",
      "Epoch 487, Batch 0, Loss: 0.5500\n",
      "Epoch 488, Batch 0, Loss: 0.5977\n",
      "Epoch 489, Batch 0, Loss: 0.4115\n",
      "Epoch 490, Batch 0, Loss: 0.6003\n",
      "Epoch 491, Batch 0, Loss: 0.3527\n",
      "Epoch 492, Batch 0, Loss: 0.9186\n",
      "Epoch 493, Batch 0, Loss: 0.0342\n",
      "Epoch 494, Batch 0, Loss: 0.1344\n",
      "Epoch 495, Batch 0, Loss: 0.5104\n",
      "Epoch 496, Batch 0, Loss: 0.6108\n",
      "Epoch 497, Batch 0, Loss: 0.5140\n",
      "Epoch 498, Batch 0, Loss: 0.4352\n",
      "Epoch 499, Batch 0, Loss: 0.0638\n",
      "Epoch 500, Batch 0, Loss: 0.6032\n",
      "Epoch 501, Batch 0, Loss: 0.4278\n",
      "Epoch 502, Batch 0, Loss: 0.3816\n",
      "Epoch 503, Batch 0, Loss: 0.0814\n",
      "Epoch 504, Batch 0, Loss: 0.4003\n",
      "Epoch 505, Batch 0, Loss: 0.2311\n",
      "Epoch 506, Batch 0, Loss: 0.4530\n",
      "Epoch 507, Batch 0, Loss: 0.3763\n",
      "Epoch 508, Batch 0, Loss: 0.0337\n",
      "Epoch 509, Batch 0, Loss: 0.5448\n",
      "Epoch 510, Batch 0, Loss: 0.5265\n",
      "Epoch 511, Batch 0, Loss: 0.6355\n",
      "Epoch 512, Batch 0, Loss: 0.6022\n",
      "Epoch 513, Batch 0, Loss: 0.0408\n",
      "Epoch 514, Batch 0, Loss: 0.4357\n",
      "Epoch 515, Batch 0, Loss: 0.6628\n",
      "Epoch 516, Batch 0, Loss: 0.3001\n",
      "Epoch 517, Batch 0, Loss: 0.4893\n",
      "Epoch 518, Batch 0, Loss: 0.5118\n",
      "Epoch 519, Batch 0, Loss: 0.5735\n",
      "Epoch 520, Batch 0, Loss: 0.1447\n",
      "Epoch 521, Batch 0, Loss: 0.4002\n",
      "Epoch 522, Batch 0, Loss: 0.2360\n",
      "Epoch 523, Batch 0, Loss: 0.4784\n",
      "Epoch 524, Batch 0, Loss: 0.6430\n",
      "Epoch 525, Batch 0, Loss: 0.7216\n",
      "Epoch 526, Batch 0, Loss: 0.6983\n",
      "Epoch 527, Batch 0, Loss: 0.1569\n",
      "Epoch 528, Batch 0, Loss: 0.6855\n",
      "Epoch 529, Batch 0, Loss: 0.0825\n",
      "Epoch 530, Batch 0, Loss: 0.1079\n",
      "Epoch 531, Batch 0, Loss: 0.0165\n",
      "Epoch 532, Batch 0, Loss: 0.5347\n",
      "Epoch 533, Batch 0, Loss: 0.1249\n",
      "Epoch 534, Batch 0, Loss: 0.4797\n",
      "Epoch 535, Batch 0, Loss: 0.0099\n",
      "Epoch 536, Batch 0, Loss: 0.1770\n",
      "Epoch 537, Batch 0, Loss: 0.2053\n",
      "Epoch 538, Batch 0, Loss: 0.0131\n",
      "Epoch 539, Batch 0, Loss: 0.1285\n",
      "Epoch 540, Batch 0, Loss: 0.0254\n",
      "Epoch 541, Batch 0, Loss: 0.4255\n",
      "Epoch 542, Batch 0, Loss: 0.4635\n",
      "Epoch 543, Batch 0, Loss: 0.0046\n",
      "Epoch 544, Batch 0, Loss: 0.0025\n",
      "Epoch 545, Batch 0, Loss: 0.1737\n",
      "Epoch 546, Batch 0, Loss: 0.1371\n",
      "Epoch 547, Batch 0, Loss: 0.3443\n",
      "Epoch 548, Batch 0, Loss: 0.0151\n",
      "Epoch 549, Batch 0, Loss: 0.0226\n",
      "Epoch 550, Batch 0, Loss: 0.1495\n",
      "Epoch 551, Batch 0, Loss: 0.0016\n",
      "Epoch 552, Batch 0, Loss: 0.4310\n",
      "Epoch 553, Batch 0, Loss: 0.2843\n",
      "Epoch 554, Batch 0, Loss: 0.0159\n",
      "Epoch 555, Batch 0, Loss: 0.0890\n",
      "Epoch 556, Batch 0, Loss: 0.3771\n",
      "Epoch 557, Batch 0, Loss: 0.2464\n",
      "Epoch 558, Batch 0, Loss: 0.3098\n",
      "Epoch 559, Batch 0, Loss: 0.0072\n",
      "Epoch 560, Batch 0, Loss: 0.1489\n",
      "Epoch 561, Batch 0, Loss: 0.0063\n",
      "Epoch 562, Batch 0, Loss: 0.5060\n",
      "Epoch 563, Batch 0, Loss: 0.5114\n",
      "Epoch 564, Batch 0, Loss: 0.0224\n",
      "Epoch 565, Batch 0, Loss: 0.0439\n",
      "Epoch 566, Batch 0, Loss: 0.5523\n",
      "Epoch 567, Batch 0, Loss: 0.0851\n",
      "Epoch 568, Batch 0, Loss: 0.0877\n",
      "Epoch 569, Batch 0, Loss: 0.4631\n",
      "Epoch 570, Batch 0, Loss: 0.2557\n",
      "Epoch 571, Batch 0, Loss: 0.8535\n",
      "Epoch 572, Batch 0, Loss: 0.1268\n",
      "Epoch 573, Batch 0, Loss: 0.3500\n",
      "Epoch 574, Batch 0, Loss: 0.6628\n",
      "Epoch 575, Batch 0, Loss: 0.5487\n",
      "Epoch 576, Batch 0, Loss: 0.1750\n",
      "Epoch 577, Batch 0, Loss: 0.5339\n",
      "Epoch 578, Batch 0, Loss: 0.3829\n",
      "Epoch 579, Batch 0, Loss: 0.4569\n",
      "Epoch 580, Batch 0, Loss: 0.0341\n",
      "Epoch 581, Batch 0, Loss: 0.0529\n",
      "Epoch 582, Batch 0, Loss: 0.3440\n",
      "Epoch 583, Batch 0, Loss: 0.0795\n",
      "Epoch 584, Batch 0, Loss: 0.1112\n",
      "Epoch 585, Batch 0, Loss: 0.0726\n",
      "Epoch 586, Batch 0, Loss: 0.6242\n",
      "Epoch 587, Batch 0, Loss: 0.0293\n",
      "Epoch 588, Batch 0, Loss: 0.0974\n",
      "Epoch 589, Batch 0, Loss: 0.5092\n",
      "Epoch 590, Batch 0, Loss: 0.6143\n",
      "Epoch 591, Batch 0, Loss: 0.3275\n",
      "Epoch 592, Batch 0, Loss: 0.0702\n",
      "Epoch 593, Batch 0, Loss: 0.3120\n",
      "Epoch 594, Batch 0, Loss: 0.3121\n",
      "Epoch 595, Batch 0, Loss: 0.0690\n",
      "Epoch 596, Batch 0, Loss: 0.5019\n",
      "Epoch 597, Batch 0, Loss: 0.1045\n",
      "Epoch 598, Batch 0, Loss: 0.0363\n",
      "Epoch 599, Batch 0, Loss: 1.1958\n",
      "Epoch 600, Batch 0, Loss: 0.5905\n",
      "Epoch 601, Batch 0, Loss: 0.1402\n",
      "Epoch 602, Batch 0, Loss: 0.0809\n",
      "Epoch 603, Batch 0, Loss: 0.1231\n",
      "Epoch 604, Batch 0, Loss: 0.8088\n",
      "Epoch 605, Batch 0, Loss: 0.6487\n",
      "Epoch 606, Batch 0, Loss: 0.3389\n",
      "Epoch 607, Batch 0, Loss: 0.4682\n",
      "Epoch 608, Batch 0, Loss: 0.7178\n",
      "Epoch 609, Batch 0, Loss: 0.2920\n",
      "Epoch 610, Batch 0, Loss: 0.3534\n",
      "Epoch 611, Batch 0, Loss: 0.0345\n",
      "Epoch 612, Batch 0, Loss: 0.6294\n",
      "Epoch 613, Batch 0, Loss: 0.0789\n",
      "Epoch 614, Batch 0, Loss: 0.5203\n",
      "Epoch 615, Batch 0, Loss: 0.0941\n",
      "Epoch 616, Batch 0, Loss: 0.5026\n",
      "Epoch 617, Batch 0, Loss: 0.3452\n",
      "Epoch 618, Batch 0, Loss: 0.6520\n",
      "Epoch 619, Batch 0, Loss: 0.1029\n",
      "Epoch 620, Batch 0, Loss: 0.0757\n",
      "Epoch 621, Batch 0, Loss: 0.0083\n",
      "Epoch 622, Batch 0, Loss: 0.0152\n",
      "Epoch 623, Batch 0, Loss: 0.0185\n",
      "Epoch 624, Batch 0, Loss: 0.0058\n",
      "Epoch 625, Batch 0, Loss: 0.0118\n",
      "Epoch 626, Batch 0, Loss: 0.0860\n",
      "Epoch 627, Batch 0, Loss: 0.0499\n",
      "Epoch 628, Batch 0, Loss: 0.0055\n",
      "Epoch 629, Batch 0, Loss: 0.0279\n",
      "Epoch 630, Batch 0, Loss: 0.0094\n",
      "Epoch 631, Batch 0, Loss: 0.1202\n",
      "Epoch 632, Batch 0, Loss: 0.0516\n",
      "Epoch 633, Batch 0, Loss: 0.0698\n",
      "Epoch 634, Batch 0, Loss: 0.0483\n",
      "Epoch 635, Batch 0, Loss: 0.0441\n",
      "Epoch 636, Batch 0, Loss: 0.0514\n",
      "Epoch 637, Batch 0, Loss: 0.0055\n",
      "Epoch 638, Batch 0, Loss: 0.0039\n",
      "Epoch 639, Batch 0, Loss: 0.0092\n",
      "Epoch 640, Batch 0, Loss: 0.0056\n",
      "Epoch 641, Batch 0, Loss: 0.0023\n",
      "Epoch 642, Batch 0, Loss: 0.0133\n",
      "Epoch 643, Batch 0, Loss: 0.0029\n",
      "Epoch 644, Batch 0, Loss: 0.0051\n",
      "Epoch 645, Batch 0, Loss: 0.0005\n",
      "Epoch 646, Batch 0, Loss: 0.0002\n",
      "Epoch 647, Batch 0, Loss: 0.0006\n",
      "Epoch 648, Batch 0, Loss: 0.0168\n",
      "Epoch 649, Batch 0, Loss: 0.0007\n",
      "Epoch 650, Batch 0, Loss: 0.0004\n",
      "Epoch 651, Batch 0, Loss: 0.0063\n",
      "Epoch 652, Batch 0, Loss: 0.0008\n",
      "Epoch 653, Batch 0, Loss: 0.0007\n",
      "Epoch 654, Batch 0, Loss: 0.0001\n",
      "Epoch 655, Batch 0, Loss: 0.0027\n",
      "Epoch 656, Batch 0, Loss: 0.0014\n",
      "Epoch 657, Batch 0, Loss: 0.0041\n",
      "Epoch 658, Batch 0, Loss: 0.0005\n",
      "Epoch 659, Batch 0, Loss: 0.0002\n",
      "Epoch 660, Batch 0, Loss: 0.0002\n",
      "Epoch 661, Batch 0, Loss: 0.0004\n",
      "Epoch 662, Batch 0, Loss: 0.0011\n",
      "Epoch 663, Batch 0, Loss: 0.0015\n",
      "Epoch 664, Batch 0, Loss: 0.0014\n",
      "Epoch 665, Batch 0, Loss: 0.0004\n",
      "Epoch 666, Batch 0, Loss: 0.0008\n",
      "Epoch 667, Batch 0, Loss: 0.0010\n",
      "Epoch 668, Batch 0, Loss: 0.0001\n",
      "Epoch 669, Batch 0, Loss: 0.0001\n",
      "Epoch 670, Batch 0, Loss: 0.0002\n",
      "Epoch 671, Batch 0, Loss: 0.0010\n",
      "Epoch 672, Batch 0, Loss: 0.0000\n",
      "Epoch 673, Batch 0, Loss: 0.0002\n",
      "Epoch 674, Batch 0, Loss: 0.0000\n",
      "Epoch 675, Batch 0, Loss: 0.0000\n",
      "Epoch 676, Batch 0, Loss: 0.0002\n",
      "Epoch 677, Batch 0, Loss: 0.0002\n",
      "Epoch 678, Batch 0, Loss: 0.0001\n",
      "Epoch 679, Batch 0, Loss: 0.0004\n",
      "Epoch 680, Batch 0, Loss: 0.0001\n",
      "Epoch 681, Batch 0, Loss: 0.0021\n",
      "Epoch 682, Batch 0, Loss: 0.0123\n",
      "Epoch 683, Batch 0, Loss: 0.0008\n",
      "Epoch 684, Batch 0, Loss: 0.0003\n",
      "Epoch 685, Batch 0, Loss: 0.0009\n",
      "Epoch 686, Batch 0, Loss: 0.0008\n",
      "Epoch 687, Batch 0, Loss: 0.0003\n",
      "Epoch 688, Batch 0, Loss: 0.0002\n",
      "Epoch 689, Batch 0, Loss: 0.0002\n",
      "Epoch 690, Batch 0, Loss: 0.0011\n",
      "Epoch 691, Batch 0, Loss: 0.0003\n",
      "Epoch 692, Batch 0, Loss: 0.0004\n",
      "Epoch 693, Batch 0, Loss: 0.0000\n",
      "Epoch 694, Batch 0, Loss: 0.0012\n",
      "Epoch 695, Batch 0, Loss: 0.0010\n",
      "Epoch 696, Batch 0, Loss: 0.0002\n",
      "Epoch 697, Batch 0, Loss: 0.0002\n",
      "Epoch 698, Batch 0, Loss: 0.0006\n",
      "Epoch 699, Batch 0, Loss: 0.0007\n",
      "Epoch 700, Batch 0, Loss: 0.0000\n",
      "Epoch 701, Batch 0, Loss: 0.0001\n",
      "Epoch 702, Batch 0, Loss: 0.0016\n",
      "Epoch 703, Batch 0, Loss: 0.0001\n",
      "Epoch 704, Batch 0, Loss: 0.0006\n",
      "Epoch 705, Batch 0, Loss: 0.0013\n",
      "Epoch 706, Batch 0, Loss: 0.0001\n",
      "Epoch 707, Batch 0, Loss: 0.0002\n",
      "Epoch 708, Batch 0, Loss: 0.0023\n",
      "Epoch 709, Batch 0, Loss: 0.0007\n",
      "Epoch 710, Batch 0, Loss: 0.0043\n",
      "Epoch 711, Batch 0, Loss: 0.0038\n",
      "Epoch 712, Batch 0, Loss: 0.0050\n",
      "Epoch 713, Batch 0, Loss: 0.0002\n",
      "Epoch 714, Batch 0, Loss: 0.0000\n",
      "Epoch 715, Batch 0, Loss: 0.0061\n",
      "Epoch 716, Batch 0, Loss: 0.0003\n",
      "Epoch 717, Batch 0, Loss: 0.0007\n",
      "Epoch 718, Batch 0, Loss: 0.0002\n",
      "Epoch 719, Batch 0, Loss: 0.0018\n",
      "Epoch 720, Batch 0, Loss: 0.0147\n",
      "Epoch 721, Batch 0, Loss: 0.0026\n",
      "Epoch 722, Batch 0, Loss: 0.0082\n",
      "Epoch 723, Batch 0, Loss: 0.0168\n",
      "Epoch 724, Batch 0, Loss: 0.0009\n",
      "Epoch 725, Batch 0, Loss: 0.0007\n",
      "Epoch 726, Batch 0, Loss: 0.0018\n",
      "Epoch 727, Batch 0, Loss: 0.0240\n",
      "Epoch 728, Batch 0, Loss: 0.0017\n",
      "Epoch 729, Batch 0, Loss: 0.0050\n",
      "Epoch 730, Batch 0, Loss: 0.0038\n",
      "Epoch 731, Batch 0, Loss: 0.0205\n",
      "Epoch 732, Batch 0, Loss: 0.0100\n",
      "Epoch 733, Batch 0, Loss: 0.0013\n",
      "Epoch 734, Batch 0, Loss: 0.0043\n",
      "Epoch 735, Batch 0, Loss: 0.0025\n",
      "Epoch 736, Batch 0, Loss: 0.0006\n",
      "Epoch 737, Batch 0, Loss: 0.0000\n",
      "Epoch 738, Batch 0, Loss: 0.0005\n",
      "Epoch 739, Batch 0, Loss: 0.0002\n",
      "Epoch 740, Batch 0, Loss: 0.0001\n",
      "Epoch 741, Batch 0, Loss: 0.0092\n",
      "Epoch 742, Batch 0, Loss: 0.0002\n",
      "Epoch 743, Batch 0, Loss: 0.0003\n",
      "Epoch 744, Batch 0, Loss: 0.0006\n",
      "Epoch 745, Batch 0, Loss: 0.0004\n",
      "Epoch 746, Batch 0, Loss: 0.0020\n",
      "Epoch 747, Batch 0, Loss: 0.0003\n",
      "Epoch 748, Batch 0, Loss: 0.0011\n",
      "Epoch 749, Batch 0, Loss: 0.0001\n",
      "Epoch 750, Batch 0, Loss: 0.0004\n",
      "Epoch 751, Batch 0, Loss: 0.0012\n",
      "Epoch 752, Batch 0, Loss: 0.0010\n",
      "Epoch 753, Batch 0, Loss: 0.0003\n",
      "Epoch 754, Batch 0, Loss: 0.0001\n",
      "Epoch 755, Batch 0, Loss: 0.0012\n",
      "Epoch 756, Batch 0, Loss: 0.0012\n",
      "Epoch 757, Batch 0, Loss: 0.0001\n",
      "Epoch 758, Batch 0, Loss: 0.0005\n",
      "Epoch 759, Batch 0, Loss: 0.0001\n",
      "Epoch 760, Batch 0, Loss: 0.0007\n",
      "Epoch 761, Batch 0, Loss: 0.0002\n",
      "Epoch 762, Batch 0, Loss: 0.0009\n",
      "Epoch 763, Batch 0, Loss: 0.0008\n",
      "Epoch 764, Batch 0, Loss: 0.0002\n",
      "Epoch 765, Batch 0, Loss: 0.0004\n",
      "Epoch 766, Batch 0, Loss: 0.0000\n",
      "Epoch 767, Batch 0, Loss: 0.0002\n",
      "Epoch 768, Batch 0, Loss: 0.0000\n",
      "Epoch 769, Batch 0, Loss: 0.0000\n",
      "Epoch 770, Batch 0, Loss: 0.0000\n",
      "Epoch 771, Batch 0, Loss: 0.0000\n",
      "Epoch 772, Batch 0, Loss: 0.0000\n",
      "Epoch 773, Batch 0, Loss: 0.0002\n",
      "Epoch 774, Batch 0, Loss: 0.0001\n",
      "Epoch 775, Batch 0, Loss: 0.0000\n",
      "Epoch 776, Batch 0, Loss: 0.0000\n",
      "Epoch 777, Batch 0, Loss: 0.0000\n",
      "Epoch 778, Batch 0, Loss: 0.0002\n",
      "Epoch 779, Batch 0, Loss: 0.0001\n",
      "Epoch 780, Batch 0, Loss: 0.0000\n",
      "Epoch 781, Batch 0, Loss: 0.0000\n",
      "Epoch 782, Batch 0, Loss: 0.0001\n",
      "Epoch 783, Batch 0, Loss: 0.0000\n",
      "Epoch 784, Batch 0, Loss: 0.0000\n",
      "Epoch 785, Batch 0, Loss: 0.0000\n",
      "Epoch 786, Batch 0, Loss: 0.0001\n",
      "Epoch 787, Batch 0, Loss: 0.0001\n",
      "Epoch 788, Batch 0, Loss: 0.0003\n",
      "Epoch 789, Batch 0, Loss: 0.0008\n",
      "Epoch 790, Batch 0, Loss: 0.0010\n",
      "Epoch 791, Batch 0, Loss: 0.0004\n",
      "Epoch 792, Batch 0, Loss: 0.0015\n",
      "Epoch 793, Batch 0, Loss: 0.0087\n",
      "Epoch 794, Batch 0, Loss: 0.0073\n",
      "Epoch 795, Batch 0, Loss: 0.0074\n",
      "Epoch 796, Batch 0, Loss: 0.0023\n",
      "Epoch 797, Batch 0, Loss: 0.0336\n",
      "Epoch 798, Batch 0, Loss: 0.0064\n",
      "Epoch 799, Batch 0, Loss: 0.0518\n",
      "Epoch 800, Batch 0, Loss: 0.0037\n",
      "Epoch 801, Batch 0, Loss: 0.0085\n",
      "Epoch 802, Batch 0, Loss: 0.0055\n",
      "Epoch 803, Batch 0, Loss: 0.0078\n",
      "Epoch 804, Batch 0, Loss: 0.0148\n",
      "Epoch 805, Batch 0, Loss: 0.0023\n",
      "Epoch 806, Batch 0, Loss: 0.0021\n",
      "Epoch 807, Batch 0, Loss: 0.0016\n",
      "Epoch 808, Batch 0, Loss: 0.0025\n",
      "Epoch 809, Batch 0, Loss: 0.0050\n",
      "Epoch 810, Batch 0, Loss: 0.0105\n",
      "Epoch 811, Batch 0, Loss: 0.0034\n",
      "Epoch 812, Batch 0, Loss: 0.0181\n",
      "Epoch 813, Batch 0, Loss: 0.0052\n",
      "Epoch 814, Batch 0, Loss: 0.0083\n",
      "Epoch 815, Batch 0, Loss: 0.0456\n",
      "Epoch 816, Batch 0, Loss: 0.0798\n",
      "Epoch 817, Batch 0, Loss: 0.0647\n",
      "Epoch 818, Batch 0, Loss: 0.0497\n",
      "Epoch 819, Batch 0, Loss: 0.3483\n",
      "Epoch 820, Batch 0, Loss: 2.5429\n",
      "Epoch 821, Batch 0, Loss: 0.7231\n",
      "Epoch 822, Batch 0, Loss: 0.4338\n",
      "Epoch 823, Batch 0, Loss: 0.5559\n",
      "Epoch 824, Batch 0, Loss: 0.3770\n",
      "Epoch 825, Batch 0, Loss: 0.8745\n",
      "Epoch 826, Batch 0, Loss: 0.8903\n",
      "Epoch 827, Batch 0, Loss: 0.4580\n",
      "Epoch 828, Batch 0, Loss: 0.4490\n",
      "Epoch 829, Batch 0, Loss: 0.6010\n",
      "Epoch 830, Batch 0, Loss: 0.1296\n",
      "Epoch 831, Batch 0, Loss: 0.2332\n",
      "Epoch 832, Batch 0, Loss: 0.3701\n",
      "Epoch 833, Batch 0, Loss: 0.5139\n",
      "Epoch 834, Batch 0, Loss: 0.1606\n",
      "Epoch 835, Batch 0, Loss: 0.3714\n",
      "Epoch 836, Batch 0, Loss: 0.0047\n",
      "Epoch 837, Batch 0, Loss: 0.2851\n",
      "Epoch 838, Batch 0, Loss: 0.2442\n",
      "Epoch 839, Batch 0, Loss: 0.6666\n",
      "Epoch 840, Batch 0, Loss: 0.1661\n",
      "Epoch 841, Batch 0, Loss: 0.5667\n",
      "Epoch 842, Batch 0, Loss: 0.4873\n",
      "Epoch 843, Batch 0, Loss: 0.7767\n",
      "Epoch 844, Batch 0, Loss: 0.0345\n",
      "Epoch 845, Batch 0, Loss: 0.3327\n",
      "Epoch 846, Batch 0, Loss: 0.4514\n",
      "Epoch 847, Batch 0, Loss: 0.4334\n",
      "Epoch 848, Batch 0, Loss: 0.0155\n",
      "Epoch 849, Batch 0, Loss: 0.0369\n",
      "Epoch 850, Batch 0, Loss: 0.0961\n",
      "Epoch 851, Batch 0, Loss: 0.0418\n",
      "Epoch 852, Batch 0, Loss: 0.0523\n",
      "Epoch 853, Batch 0, Loss: 0.1612\n",
      "Epoch 854, Batch 0, Loss: 0.4121\n",
      "Epoch 855, Batch 0, Loss: 0.0434\n",
      "Epoch 856, Batch 0, Loss: 0.5244\n",
      "Epoch 857, Batch 0, Loss: 0.2264\n",
      "Epoch 858, Batch 0, Loss: 0.0106\n",
      "Epoch 859, Batch 0, Loss: 0.0168\n",
      "Epoch 860, Batch 0, Loss: 0.0687\n",
      "Epoch 861, Batch 0, Loss: 0.1832\n",
      "Epoch 862, Batch 0, Loss: 0.0461\n",
      "Epoch 863, Batch 0, Loss: 0.0163\n",
      "Epoch 864, Batch 0, Loss: 0.0059\n",
      "Epoch 865, Batch 0, Loss: 0.1292\n",
      "Epoch 866, Batch 0, Loss: 0.0404\n",
      "Epoch 867, Batch 0, Loss: 0.0471\n",
      "Epoch 868, Batch 0, Loss: 0.0248\n",
      "Epoch 869, Batch 0, Loss: 0.0189\n",
      "Epoch 870, Batch 0, Loss: 0.0082\n",
      "Epoch 871, Batch 0, Loss: 0.0150\n",
      "Epoch 872, Batch 0, Loss: 0.0004\n",
      "Epoch 873, Batch 0, Loss: 0.0002\n",
      "Epoch 874, Batch 0, Loss: 0.0008\n",
      "Epoch 875, Batch 0, Loss: 0.0001\n",
      "Epoch 876, Batch 0, Loss: 0.0003\n",
      "Epoch 877, Batch 0, Loss: 0.0003\n",
      "Epoch 878, Batch 0, Loss: 0.0002\n",
      "Epoch 879, Batch 0, Loss: 0.0000\n",
      "Epoch 880, Batch 0, Loss: 0.0001\n",
      "Epoch 881, Batch 0, Loss: 0.0001\n",
      "Epoch 882, Batch 0, Loss: 0.0000\n",
      "Epoch 883, Batch 0, Loss: 0.0000\n",
      "Epoch 884, Batch 0, Loss: 0.0000\n",
      "Epoch 885, Batch 0, Loss: 0.0000\n",
      "Epoch 886, Batch 0, Loss: 0.0000\n",
      "Epoch 887, Batch 0, Loss: 0.0000\n",
      "Epoch 888, Batch 0, Loss: 0.0000\n",
      "Epoch 889, Batch 0, Loss: 0.0000\n",
      "Epoch 890, Batch 0, Loss: 0.0000\n",
      "Epoch 891, Batch 0, Loss: 0.0000\n",
      "Epoch 892, Batch 0, Loss: 0.0000\n",
      "Epoch 893, Batch 0, Loss: 0.0000\n",
      "Epoch 894, Batch 0, Loss: 0.0000\n",
      "Epoch 895, Batch 0, Loss: 0.0000\n",
      "Epoch 896, Batch 0, Loss: 0.0000\n",
      "Epoch 897, Batch 0, Loss: 0.0000\n",
      "Epoch 898, Batch 0, Loss: 0.0000\n",
      "Epoch 899, Batch 0, Loss: 0.0000\n",
      "Epoch 900, Batch 0, Loss: 0.0000\n",
      "Epoch 901, Batch 0, Loss: 0.0000\n",
      "Epoch 902, Batch 0, Loss: 0.0000\n",
      "Epoch 903, Batch 0, Loss: 0.0000\n",
      "Epoch 904, Batch 0, Loss: 0.0000\n",
      "Epoch 905, Batch 0, Loss: 0.0000\n",
      "Epoch 906, Batch 0, Loss: 0.0000\n",
      "Epoch 907, Batch 0, Loss: 0.0000\n",
      "Epoch 908, Batch 0, Loss: 0.0000\n",
      "Epoch 909, Batch 0, Loss: 0.0000\n",
      "Epoch 910, Batch 0, Loss: 0.0000\n",
      "Epoch 911, Batch 0, Loss: 0.0000\n",
      "Epoch 912, Batch 0, Loss: 0.0000\n",
      "Epoch 913, Batch 0, Loss: 0.0000\n",
      "Epoch 914, Batch 0, Loss: 0.0000\n",
      "Epoch 915, Batch 0, Loss: 0.0000\n",
      "Epoch 916, Batch 0, Loss: 0.0000\n",
      "Epoch 917, Batch 0, Loss: 0.0000\n",
      "Epoch 918, Batch 0, Loss: 0.0000\n",
      "Epoch 919, Batch 0, Loss: 0.0000\n",
      "Epoch 920, Batch 0, Loss: 0.0000\n",
      "Epoch 921, Batch 0, Loss: 0.0000\n",
      "Epoch 922, Batch 0, Loss: 0.0000\n",
      "Epoch 923, Batch 0, Loss: 0.0000\n",
      "Epoch 924, Batch 0, Loss: 0.0000\n",
      "Epoch 925, Batch 0, Loss: 0.0000\n",
      "Epoch 926, Batch 0, Loss: 0.0000\n",
      "Epoch 927, Batch 0, Loss: 0.0000\n",
      "Epoch 928, Batch 0, Loss: 0.0000\n",
      "Epoch 929, Batch 0, Loss: 0.0000\n",
      "Epoch 930, Batch 0, Loss: 0.0000\n",
      "Epoch 931, Batch 0, Loss: 0.0000\n",
      "Epoch 932, Batch 0, Loss: 0.0000\n",
      "Epoch 933, Batch 0, Loss: 0.0001\n",
      "Epoch 934, Batch 0, Loss: 0.0000\n",
      "Epoch 935, Batch 0, Loss: 0.0000\n",
      "Epoch 936, Batch 0, Loss: 0.0000\n",
      "Epoch 937, Batch 0, Loss: 0.0000\n",
      "Epoch 938, Batch 0, Loss: 0.0000\n",
      "Epoch 939, Batch 0, Loss: 0.0001\n",
      "Epoch 940, Batch 0, Loss: 0.0003\n",
      "Epoch 941, Batch 0, Loss: 0.0001\n",
      "Epoch 942, Batch 0, Loss: 0.0000\n",
      "Epoch 943, Batch 0, Loss: 0.0001\n",
      "Epoch 944, Batch 0, Loss: 0.0000\n",
      "Epoch 945, Batch 0, Loss: 0.0001\n",
      "Epoch 946, Batch 0, Loss: 0.0001\n",
      "Epoch 947, Batch 0, Loss: 0.0005\n",
      "Epoch 948, Batch 0, Loss: 0.0004\n",
      "Epoch 949, Batch 0, Loss: 0.0014\n",
      "Epoch 950, Batch 0, Loss: 0.0003\n",
      "Epoch 951, Batch 0, Loss: 0.0008\n",
      "Epoch 952, Batch 0, Loss: 0.0012\n",
      "Epoch 953, Batch 0, Loss: 0.0002\n",
      "Epoch 954, Batch 0, Loss: 0.0000\n",
      "Epoch 955, Batch 0, Loss: 0.0001\n",
      "Epoch 956, Batch 0, Loss: 0.0002\n",
      "Epoch 957, Batch 0, Loss: 0.0002\n",
      "Epoch 958, Batch 0, Loss: 0.0038\n",
      "Epoch 959, Batch 0, Loss: 0.0053\n",
      "Epoch 960, Batch 0, Loss: 0.0444\n",
      "Epoch 961, Batch 0, Loss: 0.0103\n",
      "Epoch 962, Batch 0, Loss: 0.0203\n",
      "Epoch 963, Batch 0, Loss: 0.0030\n",
      "Epoch 964, Batch 0, Loss: 0.0030\n",
      "Epoch 965, Batch 0, Loss: 0.0442\n",
      "Epoch 966, Batch 0, Loss: 0.5326\n",
      "Epoch 967, Batch 0, Loss: 0.0053\n",
      "Epoch 968, Batch 0, Loss: 0.0035\n",
      "Epoch 969, Batch 0, Loss: 0.0439\n",
      "Epoch 970, Batch 0, Loss: 0.4073\n",
      "Epoch 971, Batch 0, Loss: 0.0452\n",
      "Epoch 972, Batch 0, Loss: 0.1258\n",
      "Epoch 973, Batch 0, Loss: 0.0110\n",
      "Epoch 974, Batch 0, Loss: 0.0185\n",
      "Epoch 975, Batch 0, Loss: 0.0011\n",
      "Epoch 976, Batch 0, Loss: 0.0044\n",
      "Epoch 977, Batch 0, Loss: 0.0054\n",
      "Epoch 978, Batch 0, Loss: 0.0064\n",
      "Epoch 979, Batch 0, Loss: 0.0054\n",
      "Epoch 980, Batch 0, Loss: 0.0001\n",
      "Epoch 981, Batch 0, Loss: 0.0027\n",
      "Epoch 982, Batch 0, Loss: 0.0035\n",
      "Epoch 983, Batch 0, Loss: 0.0001\n",
      "Epoch 984, Batch 0, Loss: 0.0008\n",
      "Epoch 985, Batch 0, Loss: 0.0006\n",
      "Epoch 986, Batch 0, Loss: 0.0006\n",
      "Epoch 987, Batch 0, Loss: 0.0013\n",
      "Epoch 988, Batch 0, Loss: 0.0015\n",
      "Epoch 989, Batch 0, Loss: 0.0013\n",
      "Epoch 990, Batch 0, Loss: 0.0062\n",
      "Epoch 991, Batch 0, Loss: 0.0062\n",
      "Epoch 992, Batch 0, Loss: 0.0007\n",
      "Epoch 993, Batch 0, Loss: 0.0000\n",
      "Epoch 994, Batch 0, Loss: 0.0000\n",
      "Epoch 995, Batch 0, Loss: 0.0001\n",
      "Epoch 996, Batch 0, Loss: 0.0000\n",
      "Epoch 997, Batch 0, Loss: 0.0000\n",
      "Epoch 998, Batch 0, Loss: 0.0001\n",
      "Epoch 999, Batch 0, Loss: 0.0001\n",
      "Epoch 1000, Batch 0, Loss: 0.0000\n",
      "Epoch 1001, Batch 0, Loss: 0.0001\n",
      "Epoch 1002, Batch 0, Loss: 0.0000\n",
      "Epoch 1003, Batch 0, Loss: 0.0000\n",
      "Epoch 1004, Batch 0, Loss: 0.0000\n",
      "Epoch 1005, Batch 0, Loss: 0.0000\n",
      "Epoch 1006, Batch 0, Loss: 0.0000\n",
      "Epoch 1007, Batch 0, Loss: 0.0000\n",
      "Epoch 1008, Batch 0, Loss: 0.0000\n",
      "Epoch 1009, Batch 0, Loss: 0.0000\n",
      "Epoch 1010, Batch 0, Loss: 0.0000\n",
      "Epoch 1011, Batch 0, Loss: 0.0000\n",
      "Epoch 1012, Batch 0, Loss: 0.0000\n",
      "Epoch 1013, Batch 0, Loss: 0.0000\n",
      "Epoch 1014, Batch 0, Loss: 0.0000\n",
      "Epoch 1015, Batch 0, Loss: 0.0000\n",
      "Epoch 1016, Batch 0, Loss: 0.0000\n",
      "Epoch 1017, Batch 0, Loss: 0.0000\n",
      "Epoch 1018, Batch 0, Loss: 0.0000\n",
      "Epoch 1019, Batch 0, Loss: 0.0000\n",
      "Epoch 1020, Batch 0, Loss: 0.0000\n",
      "Epoch 1021, Batch 0, Loss: 0.0000\n",
      "Epoch 1022, Batch 0, Loss: 0.0000\n",
      "Epoch 1023, Batch 0, Loss: 0.0000\n",
      "Epoch 1024, Batch 0, Loss: 0.0000\n",
      "Epoch 1025, Batch 0, Loss: 0.0000\n",
      "Epoch 1026, Batch 0, Loss: 0.0000\n",
      "Epoch 1027, Batch 0, Loss: 0.0000\n",
      "Epoch 1028, Batch 0, Loss: 0.0000\n",
      "Epoch 1029, Batch 0, Loss: 0.0001\n",
      "Epoch 1030, Batch 0, Loss: 0.0000\n",
      "Epoch 1031, Batch 0, Loss: 0.0000\n",
      "Epoch 1032, Batch 0, Loss: 0.0000\n",
      "Epoch 1033, Batch 0, Loss: 0.0002\n",
      "Epoch 1034, Batch 0, Loss: 0.0002\n",
      "Epoch 1035, Batch 0, Loss: 0.0004\n",
      "Epoch 1036, Batch 0, Loss: 0.0001\n",
      "Epoch 1037, Batch 0, Loss: 0.0003\n",
      "Epoch 1038, Batch 0, Loss: 0.0000\n",
      "Epoch 1039, Batch 0, Loss: 0.0000\n",
      "Epoch 1040, Batch 0, Loss: 0.0002\n",
      "Epoch 1041, Batch 0, Loss: 0.0000\n",
      "Epoch 1042, Batch 0, Loss: 0.0000\n",
      "Epoch 1043, Batch 0, Loss: 0.0000\n",
      "Epoch 1044, Batch 0, Loss: 0.0000\n",
      "Epoch 1045, Batch 0, Loss: 0.0001\n",
      "Epoch 1046, Batch 0, Loss: 0.0001\n",
      "Epoch 1047, Batch 0, Loss: 0.0000\n",
      "Epoch 1048, Batch 0, Loss: 0.0001\n",
      "Epoch 1049, Batch 0, Loss: 0.0000\n",
      "Epoch 1050, Batch 0, Loss: 0.0000\n",
      "Epoch 1051, Batch 0, Loss: 0.0001\n",
      "Epoch 1052, Batch 0, Loss: 0.0001\n",
      "Epoch 1053, Batch 0, Loss: 0.0000\n",
      "Epoch 1054, Batch 0, Loss: 0.0001\n",
      "Epoch 1055, Batch 0, Loss: 0.0001\n",
      "Epoch 1056, Batch 0, Loss: 0.0006\n",
      "Epoch 1057, Batch 0, Loss: 0.0001\n",
      "Epoch 1058, Batch 0, Loss: 0.0001\n",
      "Epoch 1059, Batch 0, Loss: 0.0001\n",
      "Epoch 1060, Batch 0, Loss: 0.0000\n",
      "Epoch 1061, Batch 0, Loss: 0.0005\n",
      "Epoch 1062, Batch 0, Loss: 0.0014\n",
      "Epoch 1063, Batch 0, Loss: 0.0036\n",
      "Epoch 1064, Batch 0, Loss: 0.0004\n",
      "Epoch 1065, Batch 0, Loss: 0.0010\n",
      "Epoch 1066, Batch 0, Loss: 0.0000\n",
      "Epoch 1067, Batch 0, Loss: 0.0030\n",
      "Epoch 1068, Batch 0, Loss: 0.0022\n",
      "Epoch 1069, Batch 0, Loss: 0.0072\n",
      "Epoch 1070, Batch 0, Loss: 0.0026\n",
      "Epoch 1071, Batch 0, Loss: 0.0307\n",
      "Epoch 1072, Batch 0, Loss: 0.0010\n",
      "Epoch 1073, Batch 0, Loss: 0.0999\n",
      "Epoch 1074, Batch 0, Loss: 0.0107\n",
      "Epoch 1075, Batch 0, Loss: 0.0183\n",
      "Epoch 1076, Batch 0, Loss: 0.0082\n",
      "Epoch 1077, Batch 0, Loss: 0.0070\n",
      "Epoch 1078, Batch 0, Loss: 0.0007\n",
      "Epoch 1079, Batch 0, Loss: 0.0123\n",
      "Epoch 1080, Batch 0, Loss: 0.0017\n",
      "Epoch 1081, Batch 0, Loss: 0.0370\n",
      "Epoch 1082, Batch 0, Loss: 0.0062\n",
      "Epoch 1083, Batch 0, Loss: 0.0077\n",
      "Epoch 1084, Batch 0, Loss: 0.0046\n",
      "Epoch 1085, Batch 0, Loss: 0.0039\n",
      "Epoch 1086, Batch 0, Loss: 0.0004\n",
      "Epoch 1087, Batch 0, Loss: 0.0008\n",
      "Epoch 1088, Batch 0, Loss: 0.0003\n",
      "Epoch 1089, Batch 0, Loss: 0.0007\n",
      "Epoch 1090, Batch 0, Loss: 0.0002\n",
      "Epoch 1091, Batch 0, Loss: 0.0000\n",
      "Epoch 1092, Batch 0, Loss: 0.0002\n",
      "Epoch 1093, Batch 0, Loss: 0.0003\n",
      "Epoch 1094, Batch 0, Loss: 0.0000\n",
      "Epoch 1095, Batch 0, Loss: 0.0006\n",
      "Epoch 1096, Batch 0, Loss: 0.0001\n",
      "Epoch 1097, Batch 0, Loss: 0.0017\n",
      "Epoch 1098, Batch 0, Loss: 0.0006\n",
      "Epoch 1099, Batch 0, Loss: 0.0004\n",
      "Epoch 1100, Batch 0, Loss: 0.0030\n",
      "Epoch 1101, Batch 0, Loss: 0.0015\n",
      "Epoch 1102, Batch 0, Loss: 0.0007\n",
      "Epoch 1103, Batch 0, Loss: 0.0037\n",
      "Epoch 1104, Batch 0, Loss: 0.0021\n",
      "Epoch 1105, Batch 0, Loss: 0.0008\n",
      "Epoch 1106, Batch 0, Loss: 0.0092\n",
      "Epoch 1107, Batch 0, Loss: 0.0015\n",
      "Epoch 1108, Batch 0, Loss: 0.0131\n",
      "Epoch 1109, Batch 0, Loss: 0.0041\n",
      "Epoch 1110, Batch 0, Loss: 0.0052\n",
      "Epoch 1111, Batch 0, Loss: 0.0041\n",
      "Epoch 1112, Batch 0, Loss: 0.0006\n",
      "Epoch 1113, Batch 0, Loss: 0.0000\n",
      "Epoch 1114, Batch 0, Loss: 0.0002\n",
      "Epoch 1115, Batch 0, Loss: 0.0002\n",
      "Epoch 1116, Batch 0, Loss: 0.0011\n",
      "Epoch 1117, Batch 0, Loss: 0.0016\n",
      "Epoch 1118, Batch 0, Loss: 0.0191\n",
      "Epoch 1119, Batch 0, Loss: 0.0403\n",
      "Epoch 1120, Batch 0, Loss: 0.0186\n",
      "Epoch 1121, Batch 0, Loss: 0.0121\n",
      "Epoch 1122, Batch 0, Loss: 0.0063\n",
      "Epoch 1123, Batch 0, Loss: 0.0025\n",
      "Epoch 1124, Batch 0, Loss: 0.0059\n",
      "Epoch 1125, Batch 0, Loss: 0.0035\n",
      "Epoch 1126, Batch 0, Loss: 0.0240\n",
      "Epoch 1127, Batch 0, Loss: 0.0012\n",
      "Epoch 1128, Batch 0, Loss: 0.0006\n",
      "Epoch 1129, Batch 0, Loss: 0.0385\n",
      "Epoch 1130, Batch 0, Loss: 0.1620\n",
      "Epoch 1131, Batch 0, Loss: 0.0317\n",
      "Epoch 1132, Batch 0, Loss: 0.0454\n",
      "Epoch 1133, Batch 0, Loss: 1.1873\n",
      "Epoch 1134, Batch 0, Loss: 0.4091\n",
      "Epoch 1135, Batch 0, Loss: 0.8414\n",
      "Epoch 1136, Batch 0, Loss: 1.0510\n",
      "Epoch 1137, Batch 0, Loss: 0.0692\n",
      "Epoch 1138, Batch 0, Loss: 0.3029\n",
      "Epoch 1139, Batch 0, Loss: 0.1949\n",
      "Epoch 1140, Batch 0, Loss: 0.4246\n",
      "Epoch 1141, Batch 0, Loss: 0.0685\n",
      "Epoch 1142, Batch 0, Loss: 0.1666\n",
      "Epoch 1143, Batch 0, Loss: 0.1115\n",
      "Epoch 1144, Batch 0, Loss: 1.0521\n",
      "Epoch 1145, Batch 0, Loss: 0.4102\n",
      "Epoch 1146, Batch 0, Loss: 0.0510\n",
      "Epoch 1147, Batch 0, Loss: 0.1470\n",
      "Epoch 1148, Batch 0, Loss: 0.0836\n",
      "Epoch 1149, Batch 0, Loss: 0.2361\n",
      "Epoch 1150, Batch 0, Loss: 0.2445\n",
      "Epoch 1151, Batch 0, Loss: 0.0511\n",
      "Epoch 1152, Batch 0, Loss: 0.0939\n",
      "Epoch 1153, Batch 0, Loss: 0.1412\n",
      "Epoch 1154, Batch 0, Loss: 0.2566\n",
      "Epoch 1155, Batch 0, Loss: 0.3118\n",
      "Epoch 1156, Batch 0, Loss: 0.2470\n",
      "Epoch 1157, Batch 0, Loss: 0.0059\n",
      "Epoch 1158, Batch 0, Loss: 0.0847\n",
      "Epoch 1159, Batch 0, Loss: 0.0079\n",
      "Epoch 1160, Batch 0, Loss: 0.0157\n",
      "Epoch 1161, Batch 0, Loss: 0.0980\n",
      "Epoch 1162, Batch 0, Loss: 0.2476\n",
      "Epoch 1163, Batch 0, Loss: 0.0334\n",
      "Epoch 1164, Batch 0, Loss: 0.0026\n",
      "Epoch 1165, Batch 0, Loss: 0.0145\n",
      "Epoch 1166, Batch 0, Loss: 0.0014\n",
      "Epoch 1167, Batch 0, Loss: 0.0082\n",
      "Epoch 1168, Batch 0, Loss: 0.0006\n",
      "Epoch 1169, Batch 0, Loss: 0.0002\n",
      "Epoch 1170, Batch 0, Loss: 0.0018\n",
      "Epoch 1171, Batch 0, Loss: 0.0002\n",
      "Epoch 1172, Batch 0, Loss: 0.0008\n",
      "Epoch 1173, Batch 0, Loss: 0.0001\n",
      "Epoch 1174, Batch 0, Loss: 0.0013\n",
      "Epoch 1175, Batch 0, Loss: 0.0029\n",
      "Epoch 1176, Batch 0, Loss: 0.0004\n",
      "Epoch 1177, Batch 0, Loss: 0.0001\n",
      "Epoch 1178, Batch 0, Loss: 0.0003\n",
      "Epoch 1179, Batch 0, Loss: 0.0003\n",
      "Epoch 1180, Batch 0, Loss: 0.0001\n",
      "Epoch 1181, Batch 0, Loss: 0.0002\n",
      "Epoch 1182, Batch 0, Loss: 0.0001\n",
      "Epoch 1183, Batch 0, Loss: 0.0001\n",
      "Epoch 1184, Batch 0, Loss: 0.0001\n",
      "Epoch 1185, Batch 0, Loss: 0.0000\n",
      "Epoch 1186, Batch 0, Loss: 0.0001\n",
      "Epoch 1187, Batch 0, Loss: 0.0001\n",
      "Epoch 1188, Batch 0, Loss: 0.0001\n",
      "Epoch 1189, Batch 0, Loss: 0.0002\n",
      "Epoch 1190, Batch 0, Loss: 0.0003\n",
      "Epoch 1191, Batch 0, Loss: 0.0000\n",
      "Epoch 1192, Batch 0, Loss: 0.0000\n",
      "Epoch 1193, Batch 0, Loss: 0.0001\n",
      "Epoch 1194, Batch 0, Loss: 0.0000\n",
      "Epoch 1195, Batch 0, Loss: 0.0000\n",
      "Epoch 1196, Batch 0, Loss: 0.0001\n",
      "Epoch 1197, Batch 0, Loss: 0.0000\n",
      "Epoch 1198, Batch 0, Loss: 0.0000\n",
      "Epoch 1199, Batch 0, Loss: 0.0000\n",
      "Epoch 1200, Batch 0, Loss: 0.0000\n",
      "Epoch 1201, Batch 0, Loss: 0.0000\n",
      "Epoch 1202, Batch 0, Loss: 0.0001\n",
      "Epoch 1203, Batch 0, Loss: 0.0000\n",
      "Epoch 1204, Batch 0, Loss: 0.0000\n",
      "Epoch 1205, Batch 0, Loss: 0.0001\n",
      "Epoch 1206, Batch 0, Loss: 0.0001\n",
      "Epoch 1207, Batch 0, Loss: 0.0000\n",
      "Epoch 1208, Batch 0, Loss: 0.0000\n",
      "Epoch 1209, Batch 0, Loss: 0.0000\n",
      "Epoch 1210, Batch 0, Loss: 0.0000\n",
      "Epoch 1211, Batch 0, Loss: 0.0000\n",
      "Epoch 1212, Batch 0, Loss: 0.0000\n",
      "Epoch 1213, Batch 0, Loss: 0.0000\n",
      "Epoch 1214, Batch 0, Loss: 0.0000\n",
      "Epoch 1215, Batch 0, Loss: 0.0000\n",
      "Epoch 1216, Batch 0, Loss: 0.0000\n",
      "Epoch 1217, Batch 0, Loss: 0.0000\n",
      "Epoch 1218, Batch 0, Loss: 0.0001\n",
      "Epoch 1219, Batch 0, Loss: 0.0001\n",
      "Epoch 1220, Batch 0, Loss: 0.0001\n",
      "Epoch 1221, Batch 0, Loss: 0.0000\n",
      "Epoch 1222, Batch 0, Loss: 0.0000\n",
      "Epoch 1223, Batch 0, Loss: 0.0000\n",
      "Epoch 1224, Batch 0, Loss: 0.0001\n",
      "Epoch 1225, Batch 0, Loss: 0.0002\n",
      "Epoch 1226, Batch 0, Loss: 0.0002\n",
      "Epoch 1227, Batch 0, Loss: 0.0002\n",
      "Epoch 1228, Batch 0, Loss: 0.0000\n",
      "Epoch 1229, Batch 0, Loss: 0.0002\n",
      "Epoch 1230, Batch 0, Loss: 0.0004\n",
      "Epoch 1231, Batch 0, Loss: 0.0001\n",
      "Epoch 1232, Batch 0, Loss: 0.0000\n",
      "Epoch 1233, Batch 0, Loss: 0.0015\n",
      "Epoch 1234, Batch 0, Loss: 0.0013\n",
      "Epoch 1235, Batch 0, Loss: 0.0002\n",
      "Epoch 1236, Batch 0, Loss: 0.0002\n",
      "Epoch 1237, Batch 0, Loss: 0.0001\n",
      "Epoch 1238, Batch 0, Loss: 0.0000\n",
      "Epoch 1239, Batch 0, Loss: 0.0003\n",
      "Epoch 1240, Batch 0, Loss: 0.0001\n",
      "Epoch 1241, Batch 0, Loss: 0.0000\n",
      "Epoch 1242, Batch 0, Loss: 0.0002\n",
      "Epoch 1243, Batch 0, Loss: 0.0001\n",
      "Epoch 1244, Batch 0, Loss: 0.0009\n",
      "Epoch 1245, Batch 0, Loss: 0.0039\n",
      "Epoch 1246, Batch 0, Loss: 0.0028\n",
      "Epoch 1247, Batch 0, Loss: 0.0161\n",
      "Epoch 1248, Batch 0, Loss: 0.0090\n",
      "Epoch 1249, Batch 0, Loss: 0.0067\n",
      "Epoch 1250, Batch 0, Loss: 0.0424\n",
      "Epoch 1251, Batch 0, Loss: 0.0088\n",
      "Epoch 1252, Batch 0, Loss: 0.0218\n",
      "Epoch 1253, Batch 0, Loss: 0.0173\n",
      "Epoch 1254, Batch 0, Loss: 0.0989\n",
      "Epoch 1255, Batch 0, Loss: 0.1565\n",
      "Epoch 1256, Batch 0, Loss: 0.0884\n",
      "Epoch 1257, Batch 0, Loss: 0.0382\n",
      "Epoch 1258, Batch 0, Loss: 0.0107\n",
      "Epoch 1259, Batch 0, Loss: 0.0140\n",
      "Epoch 1260, Batch 0, Loss: 0.0098\n",
      "Epoch 1261, Batch 0, Loss: 0.0060\n",
      "Epoch 1262, Batch 0, Loss: 0.0017\n",
      "Epoch 1263, Batch 0, Loss: 0.0003\n",
      "Epoch 1264, Batch 0, Loss: 0.0053\n",
      "Epoch 1265, Batch 0, Loss: 0.0054\n",
      "Epoch 1266, Batch 0, Loss: 0.0035\n",
      "Epoch 1267, Batch 0, Loss: 0.0185\n",
      "Epoch 1268, Batch 0, Loss: 0.0061\n",
      "Epoch 1269, Batch 0, Loss: 0.0026\n",
      "Epoch 1270, Batch 0, Loss: 0.0206\n",
      "Epoch 1271, Batch 0, Loss: 0.0140\n",
      "Epoch 1272, Batch 0, Loss: 0.0029\n",
      "Epoch 1273, Batch 0, Loss: 0.0101\n",
      "Epoch 1274, Batch 0, Loss: 0.0027\n",
      "Epoch 1275, Batch 0, Loss: 0.0004\n",
      "Epoch 1276, Batch 0, Loss: 0.0009\n",
      "Epoch 1277, Batch 0, Loss: 0.0003\n",
      "Epoch 1278, Batch 0, Loss: 0.0020\n",
      "Epoch 1279, Batch 0, Loss: 0.0037\n",
      "Epoch 1280, Batch 0, Loss: 0.0008\n",
      "Epoch 1281, Batch 0, Loss: 0.0034\n",
      "Epoch 1282, Batch 0, Loss: 0.0019\n",
      "Epoch 1283, Batch 0, Loss: 0.0001\n",
      "Epoch 1284, Batch 0, Loss: 0.0005\n",
      "Epoch 1285, Batch 0, Loss: 0.0007\n",
      "Epoch 1286, Batch 0, Loss: 0.0002\n",
      "Epoch 1287, Batch 0, Loss: 0.0004\n",
      "Epoch 1288, Batch 0, Loss: 0.0001\n",
      "Epoch 1289, Batch 0, Loss: 0.0000\n",
      "Epoch 1290, Batch 0, Loss: 0.0002\n",
      "Epoch 1291, Batch 0, Loss: 0.0000\n",
      "Epoch 1292, Batch 0, Loss: 0.0000\n",
      "Epoch 1293, Batch 0, Loss: 0.0006\n",
      "Epoch 1294, Batch 0, Loss: 0.0010\n",
      "Epoch 1295, Batch 0, Loss: 0.0002\n",
      "Epoch 1296, Batch 0, Loss: 0.0001\n",
      "Epoch 1297, Batch 0, Loss: 0.0001\n",
      "Epoch 1298, Batch 0, Loss: 0.0026\n",
      "Epoch 1299, Batch 0, Loss: 0.0021\n",
      "Epoch 1300, Batch 0, Loss: 0.0007\n",
      "Epoch 1301, Batch 0, Loss: 0.0034\n",
      "Epoch 1302, Batch 0, Loss: 0.0032\n",
      "Epoch 1303, Batch 0, Loss: 0.0006\n",
      "Epoch 1304, Batch 0, Loss: 0.0015\n",
      "Epoch 1305, Batch 0, Loss: 0.0001\n",
      "Epoch 1306, Batch 0, Loss: 0.0049\n",
      "Epoch 1307, Batch 0, Loss: 0.0003\n",
      "Epoch 1308, Batch 0, Loss: 0.0025\n",
      "Epoch 1309, Batch 0, Loss: 0.0152\n",
      "Epoch 1310, Batch 0, Loss: 0.0287\n",
      "Epoch 1311, Batch 0, Loss: 0.0016\n",
      "Epoch 1312, Batch 0, Loss: 0.0737\n",
      "Epoch 1313, Batch 0, Loss: 0.1606\n",
      "Epoch 1314, Batch 0, Loss: 0.0430\n",
      "Epoch 1315, Batch 0, Loss: 0.0924\n",
      "Epoch 1316, Batch 0, Loss: 0.2016\n",
      "Epoch 1317, Batch 0, Loss: 0.0623\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 42\u001b[0m\n\u001b[1;32m     39\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10000\u001b[39m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, (rendered_img, ground_truth_rt_delta) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[1;32m     43\u001b[0m         rendered_img \u001b[38;5;241m=\u001b[39m rendered_img\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     44\u001b[0m         ground_truth_rt_delta \u001b[38;5;241m=\u001b[39m ground_truth_rt_delta\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    677\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    680\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[2], line 211\u001b[0m, in \u001b[0;36mPoseRefinementDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;66;03m# Load rendered image as grayscale\u001b[39;00m\n\u001b[1;32m    210\u001b[0m rendered_img_path \u001b[38;5;241m=\u001b[39m entry[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msilhouette_path\u001b[39m\u001b[38;5;124m'\u001b[39m]  \u001b[38;5;66;03m# Directly use the full path\u001b[39;00m\n\u001b[0;32m--> 211\u001b[0m rendered_img \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrendered_img_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mL\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;66;03m# Convert PIL images to tensors\u001b[39;00m\n\u001b[1;32m    214\u001b[0m rendered_img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(rendered_img)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/PIL/Image.py:937\u001b[0m, in \u001b[0;36mImage.convert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m    889\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert\u001b[39m(\n\u001b[1;32m    890\u001b[0m     \u001b[38;5;28mself\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, matrix\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dither\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, palette\u001b[38;5;241m=\u001b[39mPalette\u001b[38;5;241m.\u001b[39mWEB, colors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m\n\u001b[1;32m    891\u001b[0m ):\n\u001b[1;32m    892\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    893\u001b[0m \u001b[38;5;124;03m    Returns a converted copy of this image. For the \"P\" mode, this\u001b[39;00m\n\u001b[1;32m    894\u001b[0m \u001b[38;5;124;03m    method translates pixels through the palette.  If mode is\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    934\u001b[0m \u001b[38;5;124;03m    :returns: An :py:class:`~PIL.Image.Image` object.\u001b[39;00m\n\u001b[1;32m    935\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 937\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    939\u001b[0m     has_transparency \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransparency\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    940\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    941\u001b[0m         \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/PIL/ImageFile.py:249\u001b[0m, in \u001b[0;36mImageFile.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 249\u001b[0m         s \u001b[38;5;241m=\u001b[39m \u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecodermaxblock\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mIndexError\u001b[39;00m, struct\u001b[38;5;241m.\u001b[39merror) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    251\u001b[0m         \u001b[38;5;66;03m# truncated png/gif\u001b[39;00m\n\u001b[1;32m    252\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m LOAD_TRUNCATED_IMAGES:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/PIL/PngImagePlugin.py:980\u001b[0m, in \u001b[0;36mPngImageFile.load_read\u001b[0;34m(self, read_bytes)\u001b[0m\n\u001b[1;32m    976\u001b[0m     read_bytes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(read_bytes, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__idat)\n\u001b[1;32m    978\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__idat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__idat \u001b[38;5;241m-\u001b[39m read_bytes\n\u001b[0;32m--> 980\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mread_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "def safe_acos(x):\n",
    "    # Clamp input to the range [-1 + epsilon, 1 - epsilon]\n",
    "    eps = 1e-7\n",
    "    x = torch.clamp(x, -1 + eps, 1 - eps)\n",
    "    return torch.acos(x)\n",
    "\n",
    "def loss_fn(pred_translation,  gt_translation):\n",
    "    # Translation loss\n",
    "    translation_loss = nn.MSELoss()(pred_translation, gt_translation)\n",
    "    \n",
    "    # Ensure the predicted rotation quaternion is normalized\n",
    "    # pred_rotation = F.normalize(pred_rotation, p=2, dim=-1)\n",
    "    \n",
    "    # Rotation loss using quaternion angular distance approximation\n",
    "    # dot_product = (pred_rotation * gt_rotation).sum(dim=-1)\n",
    "    # dot_product = torch.clamp(dot_product, -1.0 + 1e-7, 1.0 - 1e-7)\n",
    "    # angular_distance = 2 * torch.acos(torch.abs(dot_product))\n",
    "    # rotation_loss = angular_distance.mean()\n",
    "\n",
    "    # Combine losses\n",
    "    total_loss = translation_loss  # rotation_loss\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "\n",
    "from torchvision import transforms\n",
    "# Define the transformation pipeline\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),  # Converts PIL Image or numpy.ndarray to tensor.\n",
    "    transforms.Normalize((0.5,), (0.5,))  # Normalize to [-1, 1] for each channel.\n",
    "])\n",
    "\n",
    "num_epochs = 10000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (rendered_img, ground_truth_rt_delta) in enumerate(dataloader):\n",
    "        rendered_img = rendered_img.to(device)\n",
    "        ground_truth_rt_delta = ground_truth_rt_delta.to(device)\n",
    "        \n",
    "        batch_size = rendered_img.shape[0]\n",
    "        reference_img_batched = reference_img_tensor.repeat(batch_size, 1, 1, 1)\n",
    "        \n",
    "        translation_update = model(rendered_img.to(device), reference_img_batched.to(device))\n",
    "\n",
    "        # Assuming your model outputs and ground_truth_rt_delta are structured correctly\n",
    "        translation_vector = ground_truth_rt_delta[:, :3, 3]\n",
    "        # Example conversion, replace with your actual conversion function\n",
    "         #quaternion = rotation_matrix_to_quaternion(ground_truth_rt_delta[:, :3, :3])\n",
    "\n",
    "        loss = loss_fn(translation_update, translation_vector)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f'Epoch {epoch+1}, Batch {i}, Loss: {loss.item():.4f}')\n",
    "\n",
    "# After training completes\n",
    "model_save_path = './pose_refine_example/pose_refine_translate.pth'\n",
    "torch.save(model.state_dict(), model_save_path)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "anp_metadata": {
   "path": "fbsource/fbcode/vision/fair/pytorch3d/docs/tutorials/camera_position_optimization_with_differentiable_rendering.ipynb"
  },
  "bento_stylesheets": {
   "bento/extensions/flow/main.css": true,
   "bento/extensions/kernel_selector/main.css": true,
   "bento/extensions/kernel_ui/main.css": true,
   "bento/extensions/new_kernel/main.css": true,
   "bento/extensions/system_usage/main.css": true,
   "bento/extensions/theme/main.css": true
  },
  "colab": {
   "name": "camera_position_optimization_with_differentiable_rendering.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "disseminate_notebook_info": {
   "backup_notebook_id": "1062179640844868"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
